# Hadoop

Install


1. References
    1. [hadoop](http://hadoop.apache.org/)
    2. [hadoop standalone](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)
    3. [hadoop cluster](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html)
    4. [zookeeper](https://zookeeper.apache.org/)
2. Java
    1. Envs

        ```sh
        export JAVA_HOME=/server/proj/hp/bin/java/jdk1.8.0_141
        export PATH=$PATH:${JAVA_HOME}/bin
        ```
    2. JDBC
        1. `mysql-connector-java`: `https://dev.mysql.com/downloads/connector/j/`
3. Hadoop
    1. Install

        ```sh
        HADOOP_VERSION=3.2.2
        wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
        tar xf hadoop-${HADOOP_VERSION}.tar.gz

        # validate
        wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}-src.tar.gz.sha512
        sha512 hadoop-${HADOOP_VERSION}.tar.gz > hadoop.sha512
        diff hadoop.sha512 hadoop-${HADOOP_VERSION}.tar.gz.sha512
        ```
    2. Envs

        ```sh
        export HADOOP_HOME=/server/proj/hp/bin/hadoop/hadoop-3.1.2
        export HADOOP_CONF_DIR=/server/proj/hp/config/hadoop/3.1.2
        export HADOOP_PID_DIR=/server/proj/hp/pid/hadoop
        export HADOOP_LOG_DIR=/server/proj/hp/log/hadoop
        export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}
        ```
    3. Configurations
        1. `hadoop-env.sh`
            1. `export JAVA_HOME=...`
        2. `masters`: specify hadoop master host, `hpmaster` for example
        3. `workers`: `slaves` for hadoop2, spiecify hadoop worker hosts, one host per line
        4. `core-site.xml`
            1. `fs.defaultFS: hdfs://hpmaster:9000`: NameNode URI for access hadoop filesystem
            2. `hadoop.tmp.dir: /server/proj/hp/tmp/hadoop/tmp`: temporary file directory
            3. `[ io.file.buffer.size: 131072 ]`: size of read/write buffer used in SequenceFiles
            4. `[dfs.blocksize: 268435456]`: HDFS blocksize of 256MB for large file-systems
        5. `hdfs-site.xml`
            1. `dfs.namenode.name.dir: file:///server/proj/hp/data/hadoop/name`
            2. `dfs.datanode.data.dir: file:///server/proj/hp/data/hadoop/data`: comma separated list of paths on the local filesystem
            3. `dfs.replication: 2`
        6. `mapred-site.xml`
            1. `mapreduce.framework.name: yarn`: execution framework set to Hadoop YARN
            2. `mapreduce.application.classpath: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*`
            3. `yarn.app.mapreduce.am.resource.mb: 2048`
            4. `mapreduce.map.memory.mb: 1024`
            5. `mapreduce.reduce.memory.mb: 1024`
            6. `[ yarn.app.mapreduce.am.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
            7. `[ mapreduce.map.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
            8. `[ mapreduce.reduce.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
        7. `yarn-site.xml`
            1. `yarn.resourcemanager.hostname: hpmaster`
            2. `yarn.nodemanager.aux-services: mapreduce_shuffle`
            3. `yarn.nodemanager.resource.memory-mb: 6144`
            4. `yarn.scheduler.maximum-allocation-mb: 6144`
            5. `yarn.scheduler.minimum-allocation-mb: 512`
            6. `[ yarn.resourcemanager.webapp.address: hpmaster:8088 ]`
            7. `[ yarn.resourcemanager.address: hpmaster:8032 ]`
            8. `[ yarn.resourcemanager.scheduler.address: hpmaster:8030 ]`
            9.  `[ yarn.resourcemanager.resource-tracker.address: hpmaster:8031 ]`
            10. `yarn.resourcemanager.scheduler.class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler`: for spark
            11. `yarn.nodemanager.vmem-check-enabled: false`: disable virtual-memory checking and prevent containers from being allocated on JDK8
            12. `yarn.nodemanager.pmem-check-enabled: false`
        8. `capacity-scheduler.xml`
            1 `[ yarn.scheduler.capacity.maximum-am-resource-percent: 0.5 ]`
        9. `log4j.properties`
    4. Service
        1. filesystem format first
            1. `hdfs namenode -format`
        2. dfs
            1. start: `$HADOOP_HOME/sbin/start-dfs.sh`
            2. stop: `$HADOOP_HOME/sbin/stop-dfs.sh`
            3. jps:
                1. master
                    1. Namenode
                    2. SecondaryNamenode
                    3. [Datanode]
                2. slave
                    1. Datanode
        3. yarn
            1. start: `$HADOOP_HOME/sbin/start-yarn.sh`
            2. stop: `$HADOOP_HOME/sbin/stop-yarn.sh`
            3. jps:
                1. master
                    1. ResourceManager
                    2. [NodeManager]
                2. slave
                    1. NodeManager
        4. problems
            1. `jps` no return value, remove file `/tmp/hsper**_username`
    5. Web UI
        1. `http://hpmaser:9870`: NameNode info, `hpmaster:50070` for hadoop2
        2. `http://hpmaster:8088`: ResourceManager info
        3. `http://hpmaster:19888`: MapReduce job history
    6. Commands
        1. `hadoop fs`: `hdfs dfs` is a synonym
            1. `-ls <path>`
                1. `scheme://authority/path`
                    1. `file:///user/hadoop`: file path
                    2. `hdfs://<namenode>:9000/path`: hdfs path
                2. relative path, the working directory is HDFS home directory `/user/<username>`, which should be created manually
            2. `-mkdir [-p] <path>`
            3. `-rm [-r] <path>`
            4. `-mv <oldpath> <newpath>`
            5. `-cat <file>`
            6. `-head <file>`
            7. `-tail <file>`
            8. `-touchz <file>`
            9. `-put <from_local_path> <to_hadoop_path>`
            10. `-appendToFile <from_local> <to_remote>`
            11. `-cp`
            12. `-get <from_hadoop_path> <to_local_path>`
            13. `-df`: display free space
            14. `-du`
            15. `-getmerge <src> <localdest>`
       1. `hadoop jar <xxx.jar>`
4. ZooKeeper
    1. Install

        ```sh
        ZOOKEEPER_VERSION=3.6.3
        wgt https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-${ZOOKEEPER_VERSION}/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz
        ```
    2. Envs
        ```sh
        EXPORT ZOOKEEPER_HOME=/data/hadoop/hadoop/bin/zookeeper/apache-zookeeper-3.6.3-bin/
        export PATH=${ZOOKEEPER_HOME}/bin:${PATH}
        ```
    2. Configurations
        1. `zoo.cfg`
            1. `tickTime=2000`: time in milliseconds to do heartbeats between servers or between client and server, the minimum session timeout will be twice the tickTime
            2. `dataDir=/path/to/data`: store in-memory database snapshots. unless specified otherwise, the transaction log of updates to the database
            3. `dataLogDir=/path/to/log`: write the transaction log to the dataLogDir rather than the dataDir
            4. `clientPort=2181`: `[default: 2181]`, listen port for client connections
            5. `initLimit=10`: need for cluster, number of ticks that follower servers initially connect to the leader server. if more than half of followers not connect to the leader within this time, then the leader will be reelected
            6. `synclimit=5`: need for cluster, ticks between sending a request and getting an acknowledgement between followers and leader. if timeout, the follower will be descarded, and the client will connect to another follower
            7. `server.x=zoo1:2888:3888`: `x` is the server id number, `2888` is for connecting to other peers, and `3888` is used for leader election
    3. Services
        1. start: `bin/zkServer.sh start [--config <conf-dir>]`: `[default: conf/zoo.cfg]`
        2. stop: `zkServer.sh stop`
        3. status: `zkServer.sh status`
        4. connect: `bin/zkCli.sh -server 127.0.0.1:2181`
