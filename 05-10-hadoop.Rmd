# Hadoop

## Environments {#hadoop-envs}

1. References
    1. [hadoop](http://hadoop.apache.org/)
    2. [hadoop standalone](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)
    3. [hadoop cluster](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html)
    4. [hadoop command](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)
    5. [zookeeper](https://zookeeper.apache.org/)
    6. [Kafka](http://kafka.apache.org/)
    7. [Remote Spark Jobs on YARN](https://theckang.github.io/2015/12/31/remote-spark-jobs-on-yarn.html)
2. Java
    1. Envs

        ```sh
        export JAVA_HOME=/server/proj/hp/bin/java/jdk1.8.0_141
        export PATH=$PATH:${JAVA_HOME}/bin
        ```
    2. JDBC
        1. mysql-connector-java: `https://dev.mysql.com/downloads/connector/j/`
        2. postgrsql: `https://jdbc.postgresql.org/download.html`
3. Hadoop
    1. Install

        ```sh
        HADOOP_VERSION=3.2.2
        wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
        tar xf hadoop-${HADOOP_VERSION}.tar.gz

        # validate
        wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}-src.tar.gz.sha512
        sha512 hadoop-${HADOOP_VERSION}.tar.gz > hadoop.sha512
        diff hadoop.sha512 hadoop-${HADOOP_VERSION}.tar.gz.sha512
        ```
    2. Envs

        ```sh
        export HADOOP_HOME=/server/proj/hp/bin/hadoop/hadoop-3.1.2
        export HADOOP_CONF_DIR=/server/proj/hp/config/hadoop/3.1.2
        export HADOOP_PID_DIR=/server/proj/hp/pid/hadoop
        export HADOOP_LOG_DIR=/server/proj/hp/log/hadoop
        export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}
        ```
    3. Configurations
        1. `hadoop-env.sh`
            1. `export JAVA_HOME=...`
        2. `masters`: specify hadoop master host, `hpmaster` for example
        3. `workers`: `slaves` for hadoop2, spiecify hadoop worker hosts, one host per line
        4. `core-site.xml`
            1. `fs.defaultFS: hdfs://hpmaster:9000`: NameNode URI for access hadoop filesystem
            2. `hadoop.tmp.dir: /server/proj/hp/tmp/hadoop/tmp`: temporary file directory
            3. `[ io.file.buffer.size: 131072 ]`: size of read/write buffer used in SequenceFiles
            4. `[dfs.blocksize: 268435456]`: HDFS blocksize of 256MB for large file-systems
        5. `hdfs-site.xml`
            1. `dfs.namenode.name.dir: file:///server/proj/hp/data/hadoop/name`
            2. `dfs.datanode.data.dir: file:///server/proj/hp/data/hadoop/data`: comma separated list of paths on the local filesystem
            3. `dfs.replication: 2`
            4. `dfs.namenode.acls.enabled: true`: enable acls for `setfacl` command
        6. `mapred-site.xml`
            1. `mapreduce.framework.name: yarn`: execution framework set to Hadoop YARN
            2. `mapreduce.application.classpath: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*`
            3. `yarn.app.mapreduce.am.resource.mb: 2048`
            4. `mapreduce.map.memory.mb: 1024`
            5. `mapreduce.reduce.memory.mb: 1024`
            6. `[ yarn.app.mapreduce.am.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
            7. `[ mapreduce.map.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
            8. `[ mapreduce.reduce.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
        7. `yarn-site.xml`
            1. `yarn.resourcemanager.hostname: hpmaster`
            2. `yarn.nodemanager.aux-services: mapreduce_shuffle`
            3. `yarn.nodemanager.resource.memory-mb: 6144`
            4. `yarn.scheduler.maximum-allocation-mb: 6144`
            5. `yarn.scheduler.minimum-allocation-mb: 512`
            6. `[ yarn.resourcemanager.webapp.address: hpmaster:8088 ]`
            7. `[ yarn.resourcemanager.address: hpmaster:8032 ]`
            8. `[ yarn.resourcemanager.scheduler.address: hpmaster:8030 ]`
            9.  `[ yarn.resourcemanager.resource-tracker.address: hpmaster:8031 ]`
            10. `yarn.resourcemanager.scheduler.class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler`: for spark
            11. `yarn.nodemanager.vmem-check-enabled: false`: disable virtual-memory checking and prevent containers from being allocated on JDK8
            12. `yarn.nodemanager.pmem-check-enabled: false`
        8. `capacity-scheduler.xml`
            1 `[ yarn.scheduler.capacity.maximum-am-resource-percent: 0.5 ]`
        9. `log4j.properties`
    4. Services
        1. filesystem format first
            1. `hdfs namenode -format`
        2. dfs
            1. start: `$HADOOP_HOME/sbin/start-dfs.sh`
            2. stop: `$HADOOP_HOME/sbin/stop-dfs.sh`
            3. jps:
                1. master
                    1. Namenode
                    2. SecondaryNamenode
                2. slave
                    1. Datanode
        3. yarn
            1. start: `$HADOOP_HOME/sbin/start-yarn.sh`
            2. stop: `$HADOOP_HOME/sbin/stop-yarn.sh`
            3. jps:
                1. master
                    1. ResourceManager
                2. slave
                    1. NodeManager
            4. application
                1. `yarn application -list`
                2. `yarn application -kill application_id`
        4. problems
            1. `jps` no return value, remove file `/tmp/hsper**_username`
    5. Access
        1. `hdfs://hpmaster:9000`: hdfs file system
        2. `http://hpmaser:9870`: NameNode info, `hpmaster:50070` for hadoop2
        3. `http://hpmaster:8088`: ResourceManager info
        4. `http://hpmaster:19888`: MapReduce job history
    6. Commands
        1. `hadoop fs`: `hdfs dfs` is a synonym
            1. `-ls <path>`
                1. `scheme://authority/path`
                    1. `file:///user/hadoop`: file path
                    2. `hdfs://<namenode>:9000/path`: hdfs path
                2. relative path, the working directory is HDFS home directory `/user/<username>`, which should be created manually
            2. `-mkdir [-p] <path>`
            3. `-rm [-r] <path>`
            4. `-mv <oldpath> <newpath>`
            5. `-cat <file>`
            6. `-head <file>`
            7. `-tail <file>`
            8. `-touchz <file>`
            9. `-put <from_local_path> <to_hadoop_path>`
            10. `-appendToFile <from_local> <to_remote>`
            11. `-cp`
            12. `-get <from_hadoop_path> <to_local_path>`
            13. `-df`: display free space
            14. `-du`
            15. `-getmerge <src> <localdest>`
            16. `-setfacl`: should set `dfs.namenode.acls.enabled: true` in `hdfs-site.xml`
                1. `hadoop fs -setfacl -m user:hadoop:rwx /user`
       1. `hadoop jar <xxx.jar>`
4. ZooKeeper
    1. Install

        ```sh
        ZOOKEEPER_VERSION=3.6.3
        wgt https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-${ZOOKEEPER_VERSION}/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz
        ```
    2. Envs
        ```sh
        EXPORT ZOOKEEPER_HOME=/data/hadoop/hadoop/bin/zookeeper/apache-zookeeper-3.6.3-bin/
        export PATH=${ZOOKEEPER_HOME}/bin:${PATH}
        ```
    3. Configurations
        1. `zoo.cfg`
            1. `tickTime=2000`: time in milliseconds to do heartbeats between servers or between client and server, the minimum session timeout will be twice the tickTime
            2. `dataDir=/path/to/data`: store in-memory database snapshots. unless specified otherwise, the transaction log of updates to the database
            3. `dataLogDir=/path/to/log`: write the transaction log to the `dataLogDir` rather than the `dataDir`
            4. `clientPort=2181`: `[default: 2181]`, listen port for client connections
            5. `initLimit=10`: need for cluster, number of ticks that follower servers initially connect to the leader server. if more than half of followers not connect to the leader within this time, then the leader will be reelected
            6. `synclimit=5`: need for cluster, ticks between sending a request and getting an acknowledgement between followers and leader. if timeout, the follower will be descarded, and the client will connect to another follower
            7. `server.x=zoo1:2888:3888`: `x` is the server id number, `2888` is for connecting to other peers, and `3888` is used for leader election
        2. `myid`
            1. for cluster
            2. put in `dataDir` directory
            3. same number `x` in `server.x` for the host
    4. Services
        1. start: `bin/zkServer.sh start [--config <conf-dir>]`: `[default: conf/zoo.cfg]`
        2. stop: `zkServer.sh stop`
        3. restart: `zkServer.sh restart`
        4. status: `zkServer.sh status`
    5. Commands
        1. connect: `bin/zkCli.sh -server 127.0.0.1:2181`
5. Kafka
    1. Install

        ```sh
        KAFKA_VERSION=2.8.0
        KAFKA_SCALA_VERSION=2.13
        wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/${KAFKA_VERSION}/kafka_${KAFKA_SCALA_VERSION}-${KAFKA_VERSION}.tgz
        ```
    2. Envs

        ```sh
        export KAFKA_HOME=/data/hadoop/hadoop/bin/kafka/kafka_2.13-2.8.0
        export PATH=${KAFKA_HOME}/bin:${PATH}
        ```
    3. Configurations
        1. `config/server.properties`
            1. `broker.id=1`: the id of broker, must be unique integer for each broker
            2. `listeners=PLAINTEXT://host:9092`: listen port for client connecting, `[default: 9092]`
            3. `log.dirs=/path/1,/path/2`: where to store log files
            4. `zookeeper.connect=zoo1:2181,zoo2:2181...`: connect to zookeeper
            5. `num.partitions=2`: partitions per topic. more partitions allow greater parallelism for consumption, but result in more files across the brokers
    4. Services
        1. start: `kafka-server-start.sh [-daemon] config/server.properties`
        2. stop: `kafka-server-stop.sh config/server.properties`
        3. topic: `kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092`
        4. jps: `kafka`
    5. Commands
        1. producer (write): `kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092`
        2. consumer (read): `kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092`
6. Scala
    1. Install

        ```sh
        SCALA_VERSION=2.12.14
        wget https://downloads.lightbend.com/scala/${SCALA_VERSION}/scala-${SCALA_VERSION}.tgz
        ```
    2. Envs

        ```sh
        export SCALA_HOME=/data/hadoop/hadoop/bin/scala/scala-2.12.14
        export PATH=${SCALA_HOME}/bin:${PATH}
        ```
7. Maven
    1. Install

        ```sh
        MAVEN_VERSION=3.8.1
        wget http://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/${MAVEN_VERSION}/binaries/apache-maven-${MAVEN_VERSION}-bin.tar.gz
        ```
    2. Envs

        ```sh
        export M2_HOME=/data/hadoop/hadoop/bin/maven/apache-maven-3.6.1
        export PATH=${M2_HOME}/bin:${PATH}
        [export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=1024m" # for java8+]
        ```
8. Spark
    1. Install

        ```sh
        SPARK_VERSION=3.1.2
        SPARK_HADOOP_VERSION=3.2
        wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz
        ```
    2. Envs

        ```sh
        export SPARK_HOME={{ spark_home }}
        export SPARK_CONF_DIR={{ spark_base.conf }}
        export SPARK_PID_DIR={{ spark_base.pid }}
        export SPARK_LOG_DIR={{ spark_base.log }}
        export PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}
        # use in the standalone mode
        export SPARK_MASTER_HOST={{ spark_master_host }}
        # when install spark free of hadoop jars, define SPARK_DIST_CLASSPATH
        # export SPARK_DIST_CLASSPATH=$(hadoop classpath)

        # avoid spark log saying ”Unable to load native-hadoop library for your platform, using builtin-java classes where applicable“  
        export LD_LIBRARY_PATH=${HADOOP_HOME}/lib/native:${LD_LIBRARY_PATH}
        ```
    3. Configurations
        1. `slaves`: spiecify spark worker hosts, one host per line
        2. `spark-defaults.conf`
            1. `spark.master yarn`
            2. `spark.driver.memory 2g`
            3. `spark.executor.memory 1g`
            4. `spark.yarn.am.memory 1g `
            5. `spark.eventLog.enabled true`
            6. `spark.eventLog.dir hdfs://{{ fs_defaultFS }}:9000/user/spark/eventlog`: directory should be create manullay

                ```sh
                hadoop fs -mkdir -p /user/spark/eventlog
                ```
            7. `spark.yarn.jars hdfs://{{ fs_defaultFS }}:9000/user/spark/jars/*`: directory should be create manullay

                ```sh
                hadoop fs -put  $SPARK_HOME/jars/* /user/spark/jars/
                ```
            8. `spark.yarn.archive hdfs://{{ fs_defaultFS_host }}:9000/user/spark/archive/spark-libs.jar`: overwrite `spark.yarn.jars`

                ```sh
                jar cv0f spark-libs.jar -C $SPARK_HOME/jars/ .
                hadoop fs -mkdir -p /user/spark/archive
                hadoop fs -put spark-libs.jar /user/spark/archive
                ```
        3. `log4j.properties`: originial `log4j.properties.template` file
    4. Services
        1. spark standalone cluster
            1. start: `$SPARK_HOME/sbin/start-all.sh`
            2. stop: `$SPARK_HOME/sbin/stop-all.sh`
            3. jps:
                1. `Master`: for master
                2. `Worker`: for slaves
        2. spark on yarn
            1. no need to start spark
            2. client config `HADOOP_CONF_DIR` including following files
                1. `core-site.xml`
                    1. `fs.defaultFS: hdfs://hpmaster:9000`: hdfs path
                2. `yarn-site.xml`
                    1. `yarn.resourcemanager.hostname: hpmaster`: yarn server
                3. `log4j.properties`: avoid `WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.`
            3. client config `SPARK_CONF_DIR` including 
                1. `spark-defaults.conf`
                    1. `spark.yarn.archive: hdfs://hpmaster:9000/user/spark/archive/spark-libs.jar`: copy from server value
            4. client config environment `LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:${LD_LIBRARY_PATH}`
        3. Problems
            1. `No route to host`: try to close the client server firewall and make sure all server can visit each other
            2. `org.apache.hadoop.security.AccessControlException`: use `setfacl` to add permission
    5. Access
        1. spark standalone cluster
            1. `spark://hpmaster:7077`: connect to spark standalone cluster
            3. `http://hpmaster:8088`: application info, hadoop yarn info
            5. `http://client:4040`: driver application info
    6. Commands
        1. `./bin/run-example SparkPi 10`
        2. `spark-shell`
            1. `spark-shell --master local[2]`: run locally with 2 threads
            2. `spark-shell --master spark://hpmaster:7077`: connect to  spark standalone cluster
            3. `spark-shell --master yarn` - connects to a yarn cluster
        3. `spark-submit`
            1. examples
                1. `spark-submit run-example SparkPi 10`
                2. `spark-submit --master yarn --deploy-mode client --class org.apache.spark.examples.SparkPi spark-examples.jar 10`
            2. arguments
                1. `--master {local[2] | spark://host:port | yarn | ...}`
                2. `--deploy-mode {client | cluster}`
                3. `--conf key=value --conf key2=value2`
                4. `--driver-memory 4g`
                5. `--num-executors 50`
                6. `--executor-memory 2g`
                7. `--executor-cores 1`
                8. `--total-executor-cores 10`
                9. `--class org.apache.spark.examples.SparkPi`:  main class
                10. `--driver-class-path postgresql-42.2.14.jar`
                11. `--jars first.jar,second.jar`: dependent jars
                12. `--py-files {.py | .zip | .egg}`: for Python, mulpile python files can be packaged into a zip or egg file
                13. `--queue thequeue`
                14. `--supervise`: for spark standalone cluster, make sure the driver restart when fail with a none-zero exit code
                15. `spark-examples.jar`: application jar
                16. `arg1 arg2 ...`: application's options
9. Hive
    1. Install

        ```sh
        HIVE_VERSION=2.3.7
        wget http://mirrors.hust.edu.cn/apache/hive/hive-${MY_HIVE_VERSION}/apache-hive-${MY_HIVE_VERSION}-bin.tar.gz
        ```
    2. Envs

        ```sh
        HIVE_HOME
        HIVE_CONF_DIR
        PATH=${HIVE_HOME}/bin:${PATH}
        ```

## Spark

1. Using Scala
    1. Initialization
        1. `import org.apache.spark.sql.SparkSession`
        2. `val spark = SparkSession.builder.appName("spark example").config("spark.config.opiton", "config-value").getOrCreate()`
    2. Configurations
        1. `spark.conf.get("spark.config.option")`
        2. `spark.conf.set("spark.config.option", "other-config-value")`
        3. `import spark.implicits._`: implicitly convert RDDs to DataFrames or use `$`notation
    3. Stop
        1. `spark.stop()`
    4. Application
        1. define a `main()` method instead of extending `scala.App`. Subclasses of `scala.App` may not work correctly

    ```scala
    import org.apache.spark.sql.SparkSession
    
    object SimpleApp {
        def main(args: Array[String]) {
            val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
            // this import is needed to use the $-notation
            import spark.implicits._
            spark.read.textFile(logFile)
            ...
            spark.stop()
        }
    }
    ```
2. Data Structure
    1. Examples

        ```scala
        case class Person(name: String, age: Long)

        val peopleRDD = spark.sparkContext.textFile("people.txt")
        val rowRDD = peopleRDD.map(_.split(",")).map(attrs => Row(attrs(0), attrs(1).trim.toInt))

        val schemaString = "name age"
        val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, nullable = true))
        val schema = StructType(fields)
        ```
    2. Cache: pull data sets into a cluster-wide in-memory cache
        1. `.cache()`
        2. `.persist()`
    3. `org.apache.spark.sql.Row`: a generic untyped JVM object
    4. `org.apache.spark.sql.types.StructField`
        1. `val field = StructField(fieldName, StringType, nullable = true]`
    5. `org.apache.spark.sql.types.StructType`
        1. `val schema = StructType(field1, field2, ...)`
    6. `org.apache.spark.rdd.RDD`
        1. Properties
            1. an immutable distributed collection partitioned across nodes
            2. strong-typed and operate in parallel with low-level API transformations and actions
        2. Initialization
            1. From File: `sparkContext` is used to create RDD
                1. `val peopleRDD = spark.sparkContext.textFile("people.txt")`
            2. From Spark
                1. `val peopleRDD = spark.sparkContext.makeRDD(1 to 5)`
                2. `val peopleRDD = spark.sparkContext.parallelize(1 t0 5)`
            3. From DataSet
                1. `val peopleRDD = peopleDS.rdd`
            4. From DataFrame
                1. `val peopleRDD = peopleDF.rdd`
        3. Transforms
            1. `.map()`: `df.map(d => d.getAs[Long]("age"))`
            2. `.flatMap()`
            3. `.filter()`
        4. Actions
            1. `.count()`
            2. `.first()`
            3. `.take(10)`
            4. `.collect()`
            5. `.reduce()`
    7. `org.apache.spark.sql.Dateset`
        1. Properties
            1. a collection of compile-time safety and strong-typed JVM objects dictated by a case class
            2. APIs are all expressed as lambda functions
            3. `Dataset[T]` typed API is optimized for data engineering tasks
        2. Initialization
            1. From Spark
                1. `spark.range(100)`
            2. From Scala
                1. `val peopleDS = Seq(Person("Andy", 32)).toDS()`
                2. `val peopleDS = spark.createDateset(List(Person("Andy", 32), Person("Bob", 12)))`
            3. From RDD
                1. `val peopleDS = peopleRDD.toDS()`
            4. From DataFrame
                1. `val peopleDS = peopleDF.as[Person]`
        3. Metadata
            1. `.columns`
            2. `.schema`
            3. `.printSchema`
            4. `.show`
            5. `.explain([true, false])`: `[default: false]`
        4. Type-Safe
            1. Transforms
                1. `.map(d => (d.name, d.age == 10))`
                2. `.flatMap()`
                3. `.filter(d => d.age === 10)`
                4. `groupByKey()`
            2. Actions
                1. `.count()`
                2. `.first()`
                3. `.take(10)`
                4. `.show([10])`
                5. `.reduce()`
        5. Untyped (aka DataFrame operations): `import spark.implicits._`
            1. `.select("name", "age"); .select($"name", $"age" + 1, $"age" > 20)`
            2. `.filter($"age" === 10)`
            3. `.groupBy("age", "name").count()`
    8. `org.apache.spark.sql.DataFrame`
        1. Properties
            1. an alias for a collection of generic objects of `Dataset[Row]` and organized into named columns
            2. faster and suitable for interactive analysis
        2. Initialization
            1. From Files
                1. `val peopleDF = spark.read.json("people.json")`
                2. `val userDF = spark.read.format("json").load("users.json")`
                3. `val userDF = spark.read.load("users.parquet")`
                4. `val userDF = spark.sql("select * from parquet.users.parquet")`
            2. From Scala
                1. `val PeopleDF = List(("Andy", 32)).toDF("name", "age")`
                2. `val PeopleDF = spark.createDataFrame(List(Person("Andy", 32), Person("Bob", 12)))`
            3. From RDD
                1. `val peopelDF = peopleRDD.toDF`
                2. `val peopelDF = spark.createDataFrame(rowRDD, schema)`
3. SQL
    1. `df.createOrReplaceTempView("people"); spark.sql("select * from people")`
    2. `df.createGlobalTempView("people"); spark.sql("select * from global_temp.people")`
    3. `spark.newSession().sql("select * from global_temp.people")`
4. Source & Sink
    1. Examples
        1. `spark.read.format("csv").option("header", "true").load("users.csv")`
        2. `usersDF.write.format("csv").option("header", "true").mode(modeValue).save("users.csv")`
        3. `usersDF.write.partitionBy("color").bucketBy(10, "name").sortBy("age").saveAsTable("t_par_but")`
    2. Path
        1. `file:///home/...`
        2. `hdfs://..`
    3. Mode: `import org.apache.spark.sql.SaveMode`
        1. `SaveMode.ErrorIfExists`: default
        2. `SaveMode.Append`
        3. `SaveMode.Overwrite`
        4. `SaveMode.Ignore`: do nothing when exists
    4. Format
        1. csv
            1. all
                1. `.option("sep", ",")`
                2. `.option("header", "true")`
            2. load
                1. `.option("inferSchema, "true")`
                2. `.option("pathGlobalFilter", "*.parquet")`
        2. json
        3. parquet
            1. load
                1. `.option("mergeSchema", "true")`
            2. write
                1. `.partitionBy("name")`
                2. `.parquet("data/test_table/key=1")`
        4. orc
            1. write
                1. `option("orc.bloom.filter.columns", "color")`
                2. `option("orc.dictionary.key.threshold", "1.0")`
                3. `option("orc.column.encoding.direct", "name")`
        5. jdbc
            1. all
                1. `.option("url", "jdbc:postgresql://host:post/dbname?user=u&password=p")`
                    1. PostgreSQL: `jdbc:postgresql://host:post/dbname?user=u&password=p`
                    2. MySQL: `jdbc:mysql://host:post/dbname?user=u&password=p`
                    3. SQLServer 2000: `jdbc:sqlserver://host:port;database=dbname;user=u;password=p`
                2. `.option("user", "username")`
                3. `.option("password", "password")`
                4. `.[option("driver","org.postgresql.Driver")]`: can omit
                5. `.option("dbtable", "schema.tablename")`
                6. `.option("numPartitions", 10)`
                7. `.jdbc("url", "schema.table", connectionProperties)`
                8. `.explain([true, false])`: execute plan
            2. load
                1. `option("query", "select * from t1")`
                2. `option("partitionColumn", "column")`
                3. `option("lowerBound", 10)`
                4. `option("upperBound", 1000)`
                5. `option("fetchsize", 100)`
                6. `option("customSchema", "id DECIMAL(38, 0), name STRING")`
            3. write
                1. `option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")`
                2. `option("batchsize", 1000)`
                3. `option("truncate", true)`: `[default: false]`, when `SaveMode.Overwrite` enabled, spark just truncate table, not drop
                4. `option("isolationLevel", "READ_UNCOMMITED")`: `[default: READ_UNCOMMITED]`
        6. libsvm
