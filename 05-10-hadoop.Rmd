# Hadoop

1. References
    1. [hadoop](http://hadoop.apache.org/)
    2. [hadoop standalone](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)
    3. [hadoop cluster](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html)
    4. [hadoop command](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)
    6. [zookeeper](https://zookeeper.apache.org/)
    7. [Kafka](http://kafka.apache.org/)
    8. [Remote Spark Jobs on YARN](https://theckang.github.io/2015/12/31/remote-spark-jobs-on-yarn.html)
2. Java
    1. Envs

        ```sh
        export JAVA_HOME=/server/proj/hp/bin/java/jdk1.8.0_141
        export PATH=$PATH:${JAVA_HOME}/bin
        ```
    2. JDBC
        1. mysql-connector-java: `https://dev.mysql.com/downloads/connector/j/`
        2. postgrsql: `https://jdbc.postgresql.org/download.html`
3. Hadoop
    1. Install

        ```sh
        HADOOP_VERSION=3.2.2
        wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
        tar xf hadoop-${HADOOP_VERSION}.tar.gz

        # validate
        wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}-src.tar.gz.sha512
        sha512 hadoop-${HADOOP_VERSION}.tar.gz > hadoop.sha512
        diff hadoop.sha512 hadoop-${HADOOP_VERSION}.tar.gz.sha512
        ```
    2. Envs

        ```sh
        export HADOOP_HOME=/server/proj/hp/bin/hadoop/hadoop-3.1.2
        export HADOOP_CONF_DIR=/server/proj/hp/config/hadoop/3.1.2
        export HADOOP_PID_DIR=/server/proj/hp/pid/hadoop
        export HADOOP_LOG_DIR=/server/proj/hp/log/hadoop
        export PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}
        ```
    3. Configurations
        1. `hadoop-env.sh`
            1. `export JAVA_HOME=...`
        2. `masters`: specify hadoop master host, `hpmaster` for example
        3. `workers`: `slaves` for hadoop2, spiecify hadoop worker hosts, one host per line
        4. `core-site.xml`
            1. `fs.defaultFS: hdfs://hpmaster:9000`: NameNode URI for access hadoop filesystem
            2. `hadoop.tmp.dir: /server/proj/hp/tmp/hadoop/tmp`: temporary file directory
            3. `[ io.file.buffer.size: 131072 ]`: size of read/write buffer used in SequenceFiles
            4. `[dfs.blocksize: 268435456]`: HDFS blocksize of 256MB for large file-systems
        5. `hdfs-site.xml`
            1. `dfs.namenode.name.dir: file:///server/proj/hp/data/hadoop/name`
            2. `dfs.datanode.data.dir: file:///server/proj/hp/data/hadoop/data`: comma separated list of paths on the local filesystem
            3. `dfs.replication: 2`
            4. `dfs.namenode.acls.enabled: true`: enable acls for `setfacl` command
        6. `mapred-site.xml`
            1. `mapreduce.framework.name: yarn`: execution framework set to Hadoop YARN
            2. `mapreduce.application.classpath: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*`
            3. `yarn.app.mapreduce.am.resource.mb: 2048`
            4. `mapreduce.map.memory.mb: 1024`
            5. `mapreduce.reduce.memory.mb: 1024`
            6. `[ yarn.app.mapreduce.am.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
            7. `[ mapreduce.map.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
            8. `[ mapreduce.reduce.env: HADOOP_MAPRED_HOME=${HADOOP_HOME ]`
        7. `yarn-site.xml`
            1. `yarn.resourcemanager.hostname: hpmaster`
            2. `yarn.nodemanager.aux-services: mapreduce_shuffle`
            3. `yarn.nodemanager.resource.memory-mb: 6144`
            4. `yarn.scheduler.maximum-allocation-mb: 6144`
            5. `yarn.scheduler.minimum-allocation-mb: 512`
            6. `[ yarn.resourcemanager.webapp.address: hpmaster:8088 ]`
            7. `[ yarn.resourcemanager.address: hpmaster:8032 ]`
            8. `[ yarn.resourcemanager.scheduler.address: hpmaster:8030 ]`
            9.  `[ yarn.resourcemanager.resource-tracker.address: hpmaster:8031 ]`
            10. `yarn.resourcemanager.scheduler.class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler`: for spark
            11. `yarn.nodemanager.vmem-check-enabled: false`: disable virtual-memory checking and prevent containers from being allocated on JDK8
            12. `yarn.nodemanager.pmem-check-enabled: false`
        8. `capacity-scheduler.xml`
            1 `[ yarn.scheduler.capacity.maximum-am-resource-percent: 0.5 ]`
        9. `log4j.properties`
    4. Services
        1. filesystem format first
            1. `hdfs namenode -format`
        2. dfs
            1. start: `$HADOOP_HOME/sbin/start-dfs.sh`
            2. stop: `$HADOOP_HOME/sbin/stop-dfs.sh`
            3. jps:
                1. master
                    1. Namenode
                    2. SecondaryNamenode
                2. slave
                    1. Datanode
        3. yarn
            1. start: `$HADOOP_HOME/sbin/start-yarn.sh`
            2. stop: `$HADOOP_HOME/sbin/stop-yarn.sh`
            3. jps:
                1. master
                    1. ResourceManager
                2. slave
                    1. NodeManager
            4. application
                1. `yarn application -list`
                2. `yarn application -kill application_id`
        4. problems
            1. `jps` no return value, remove file `/tmp/hsper**_username`
    5. Access
        1. `hdfs://hpmaster:9000`: hdfs file system
        2. `http://hpmaser:9870`: NameNode info, `hpmaster:50070` for hadoop2
        3. `http://hpmaster:8088`: ResourceManager info
        4. `http://hpmaster:19888`: MapReduce job history
    6. Commands
        1. `hadoop fs`: `hdfs dfs` is a synonym
            1. `-ls <path>`
                1. `scheme://authority/path`
                    1. `file:///user/hadoop`: file path
                    2. `hdfs://<namenode>:9000/path`: hdfs path
                2. relative path, the working directory is HDFS home directory `/user/<username>`, which should be created manually
            2. `-mkdir [-p] <path>`
            3. `-rm [-r] <path>`
            4. `-mv <oldpath> <newpath>`
            5. `-cat <file>`
            6. `-head <file>`
            7. `-tail <file>`
            8. `-touchz <file>`
            9. `-put <from_local_path> <to_hadoop_path>`
            10. `-appendToFile <from_local> <to_remote>`
            11. `-cp`
            12. `-get <from_hadoop_path> <to_local_path>`
            13. `-df`: display free space
            14. `-du`
            15. `-getmerge <src> <localdest>`
            16. `-setfacl`: should set `dfs.namenode.acls.enabled: true` in `hdfs-site.xml`
                1. `hadoop fs -setfacl -m user:hadoop:rwx /user`
       1. `hadoop jar <xxx.jar>`
4. ZooKeeper
    1. Install

        ```sh
        ZOOKEEPER_VERSION=3.6.3
        wgt https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-${ZOOKEEPER_VERSION}/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz
        ```
    2. Envs
        ```sh
        EXPORT ZOOKEEPER_HOME=/data/hadoop/hadoop/bin/zookeeper/apache-zookeeper-3.6.3-bin/
        export PATH=${ZOOKEEPER_HOME}/bin:${PATH}
        ```
    3. Configurations
        1. `zoo.cfg`
            1. `tickTime=2000`: time in milliseconds to do heartbeats between servers or between client and server, the minimum session timeout will be twice the tickTime
            2. `dataDir=/path/to/data`: store in-memory database snapshots. unless specified otherwise, the transaction log of updates to the database
            3. `dataLogDir=/path/to/log`: write the transaction log to the `dataLogDir` rather than the `dataDir`
            4. `clientPort=2181`: `[default: 2181]`, listen port for client connections
            5. `initLimit=10`: need for cluster, number of ticks that follower servers initially connect to the leader server. if more than half of followers not connect to the leader within this time, then the leader will be reelected
            6. `synclimit=5`: need for cluster, ticks between sending a request and getting an acknowledgement between followers and leader. if timeout, the follower will be descarded, and the client will connect to another follower
            7. `server.x=zoo1:2888:3888`: `x` is the server id number, `2888` is for connecting to other peers, and `3888` is used for leader election
        2. `myid`
            1. for cluster
            2. put in `dataDir` directory
            3. same number `x` in `server.x` for the host
    4. Services
        1. start: `bin/zkServer.sh start [--config <conf-dir>]`: `[default: conf/zoo.cfg]`
        2. stop: `zkServer.sh stop`
        3. restart: `zkServer.sh restart`
        4. status: `zkServer.sh status`
    5. Commands
        1. connect: `bin/zkCli.sh -server 127.0.0.1:2181`
5. Kafka
    1. Install

        ```sh
        KAFKA_VERSION=2.8.0
        KAFKA_SCALA_VERSION=2.13
        wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/${KAFKA_VERSION}/kafka_${KAFKA_SCALA_VERSION}-${KAFKA_VERSION}.tgz
        ```
    2. Envs

        ```sh
        export KAFKA_HOME=/data/hadoop/hadoop/bin/kafka/kafka_2.13-2.8.0
        export PATH=${KAFKA_HOME}/bin:${PATH}
        ```
    3. Configurations
        1. `config/server.properties`
            1. `broker.id=1`: the id of broker, must be unique integer for each broker
            2. `listeners=PLAINTEXT://host:9092`: listen port for client connecting, `[default: 9092]`
            3. `log.dirs=/path/1,/path/2`: where to store log files
            4. `zookeeper.connect=zoo1:2181,zoo2:2181...`: connect to zookeeper
            5. `num.partitions=2`: partitions per topic. more partitions allow greater parallelism for consumption, but result in more files across the brokers
    4. Services
        1. start: `kafka-server-start.sh [-daemon] config/server.properties`
        2. stop: `kafka-server-stop.sh config/server.properties`
        3. topic: `kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092`
        4. jps: `kafka`
    5. Commands
        1. producer (write): `kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092`
        2. consumer (read): `kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092`
6. Scala
    1. Install

        ```sh
        SCALA_VERSION=2.12.14
        wget https://downloads.lightbend.com/scala/${SCALA_VERSION}/scala-${SCALA_VERSION}.tgz
        ```
    2. Envs

        ```sh
        export SCALA_HOME=/data/hadoop/hadoop/bin/scala/scala-2.12.14
        export PATH=${SCALA_HOME}/bin:${PATH}
        ```
7. Maven
    1. Install

        ```sh
        MAVEN_VERSION=3.8.1
        wget http://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/${MAVEN_VERSION}/binaries/apache-maven-${MAVEN_VERSION}-bin.tar.gz
        ```
    2. Envs

        ```sh
        export M2_HOME=/data/hadoop/hadoop/bin/maven/apache-maven-3.6.1
        export PATH=${M2_HOME}/bin:${PATH}
        [export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=1024m" # for java8+]
        ```
8. Spark
    1. Install

        ```sh
        SPARK_VERSION=3.1.2
        SPARK_HADOOP_VERSION=3.2
        wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz
        ```
    2. Envs

        ```sh
        export SPARK_HOME={{ spark_home }}
        export SPARK_CONF_DIR={{ spark_base.conf }}
        export SPARK_PID_DIR={{ spark_base.pid }}
        export SPARK_LOG_DIR={{ spark_base.log }}
        export PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}
        # use in the standalone mode
        export SPARK_MASTER_HOST={{ spark_master_host }}
        # when install spark free of hadoop jars, define SPARK_DIST_CLASSPATH
        # export SPARK_DIST_CLASSPATH=$(hadoop classpath)

        # avoid spark log saying ”Unable to load native-hadoop library for your platform, using builtin-java classes where applicable“  
        export LD_LIBRARY_PATH=${HADOOP_HOME}/lib/native:${LD_LIBRARY_PATH}
        ```
    3. Configurations
        1. `slaves`: spiecify spark worker hosts, one host per line
        2. `spark-defaults.conf`
            1. `spark.master yarn`
            2. `spark.driver.memory 2g`
            3. `spark.executor.memory 1g`
            4. `spark.yarn.am.memory 1g `
            5. `spark.eventLog.enabled true`
            6. `spark.eventLog.dir hdfs://{{ fs_defaultFS }}:9000/user/spark/eventlog`: directory should be create manullay

                ```sh
                hadoop fs -mkdir -p /user/spark/eventlog
                ```
            7. `spark.yarn.jars hdfs://{{ fs_defaultFS }}:9000/user/spark/jars/*`: directory should be create manullay

                ```sh
                hadoop fs -put  $SPARK_HOME/jars/* /user/spark/jars/
                ```
            8. `spark.yarn.archive hdfs://{{ fs_defaultFS_host }}:9000/user/spark/archive/spark-libs.jar`: overwrite `spark.yarn.jars`

                ```sh
                jar cv0f spark-libs.jar -C $SPARK_HOME/jars/ .
                hadoop fs -mkdir -p /user/spark/archive
                hadoop fs -put spark-libs.jar /user/spark/archive
                ```
        3. `log4j.properties`: originial `log4j.properties.template` file
    4. Services
        1. spark standalone cluster
            1. start: `$SPARK_HOME/sbin/start-all.sh`
            2. stop: `$SPARK_HOME/sbin/stop-all.sh`
            3. jps:
                1. `Master`: for master
                2. `Worker`: for slaves
        2. spark on yarn
            1. no need to start spark
            2. client config `HADOOP_CONF_DIR` including following files
                1. `core-site.xml`
                    1. `fs.defaultFS: hdfs://hpmaster:9000`: hdfs path
                2. `yarn-site.xml`
                    1. `yarn.resourcemanager.hostname: hpmaster`: yarn server
                3. `log4j.properties`: avoid `WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.`
            3. client config `SPARK_CONF_DIR` including 
                1. `spark-defaults.conf`
                    1. `spark.yarn.archive: hdfs://hpmaster:9000/user/spark/archive/spark-libs.jar`: copy from server value
            4. client config environment `LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:${LD_LIBRARY_PATH}`
        3. Problems
            1. `No route to host`: try to close the client server firewall and make sure all server can visit each other
            2. `org.apache.hadoop.security.AccessControlException`: use `setfacl` to add permission
    5. Access
        1. spark standalone cluster
            1. `spark://hpmaster:7077`: connect to spark standalone cluster
            2. `http://hpmaster:8088`: application info, hadoop yarn info
            3. `http://<driver-node>:4040`: driver application info
    6. Commands
        1. `./bin/run-example SparkPi 10`
        2. `spark-shell`
            1. `spark-shell --master local[2]`: run locally with 2 threads
            2. `spark-shell --master spark://hpmaster:7077`: connect to  spark standalone cluster
            3. `spark-shell --master yarn` - connects to a yarn cluster
        3. `spark-submit`
            1. examples
                1. `spark-submit run-example SparkPi 10`
                2. `spark-submit --master yarn --deploy-mode client --class org.apache.spark.examples.SparkPi spark-examples.jar 10`
            2. arguments
                1. `--master {local[2] | spark://host:port | yarn | ...}`
                2. `--deploy-mode {client | cluster}`
                3. `--conf key=value --conf key2=value2`
                4. `--driver-memory 4g`
                5. `--num-executors 50`
                6. `--executor-memory 2g`
                7. `--executor-cores 1`: number of tasks (threads) running within the executor
                8. `--total-executor-cores 10`
                9. `--class org.apache.spark.examples.SparkPi`:  main class
                10. `--driver-class-path postgresql-42.2.14.jar`
                11. `--jars first.jar,second.jar`: dependent jars
                12. `--py-files {.py | .zip | .egg}`: for Python, mulpile python files can be packaged into a zip or egg file
                13. `--queue thequeue`
                14. `--supervise`: for spark standalone cluster, make sure the driver restart when fail with a none-zero exit code
                15. `spark-examples.jar`: application jar
                16. `arg1 arg2 ...`: application's options
9. Hive
    1. Install

        ```sh
        HIVE_VERSION=2.3.7
        wget http://mirrors.hust.edu.cn/apache/hive/hive-${MY_HIVE_VERSION}/apache-hive-${MY_HIVE_VERSION}-bin.tar.gz
        ```
    2. Envs

        ```sh
        HIVE_HOME
        HIVE_CONF_DIR
        PATH=${HIVE_HOME}/bin:${PATH}
        ```
