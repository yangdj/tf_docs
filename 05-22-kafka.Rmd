# Kafka

1. References
    1. [Debezium Connector](https://debezium.io/releases/1.5/)
    2. [Kafka Connectors](https://www.confluent.io/product/connectors/)
    3. [Kafka Connector REST API](https://docs.confluent.io/3.0.0/connect/userguide.html)
    4. [Debezium for MySQL Properties](https://debezium.io/documentation/reference/connectors/mysql.html#mysql-connector-properties)
2. Usage
    1. Capabilities
        1. to **publish** (write) and **subscribe** to (read) streams of events
        2. to **store** streams of events durably and reliably for as long as you want
        3. to **process** streams of events as they occur or retrospectively
    2. Functions
        1. distributed
        2. high available
        3. high scalable
        4. high throughout
        5. high performance
        6. fault-tolerant
        7. secure manner
3. Concepts
    1. Architect
        1. Servers
            1. Broker: storage layer
            2. Connector: import and export data as event streams to integrate Kafka with existing systems
        2. Clients: talk to Kafka servers
    2. Event: also called record or message, record data happened
        1. Structure
            1. events: organized and durably stored in *topics*, similar to a folder in a filesystem, and the events are the files in that folder
            2. topics:
                1. partitioned, spread over a number of buckets located on different brokers
                2. new event published to a topic, is actually appended one of the topic's partition
                3. the same event key event are written to the same partition
                4. any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written
                5. every topic can be **replicated** at the level of topic partitions. a common production replication factor is 3
            3. events are not deleted after assumption, the retaining time can be defined through a per-topic configuraiton setting
        2. Contents
            1. key
            2. value
            3. timestamp
            4. headers: optional
4. Connectors
    1. Command
        1. list connectors: `curl -X GET http://ip:port/connectors`
        2. crerate new connector: `curl -X POST -H "Content-Type:application/json" --data @mysql.properties.json http://hpmaster:8083/connectors`
        3. delete a connector: `curl -X DELETE http://ip:port/connectors/<connector-name>`
        4. restart a connector: `curl -X POST http://ip:port/connectors/<connector-name>/restart`
        5. status: `curl -X GET http://ip:port/connectors/<connector-name>/status`
        6. config: `curl -X GET http://ip:port/connectors/<connector-name>/config`
        7. config change: `curl -X PUT -H "Content-Type:application/json" http://ip:port/connectors/<connector-name>/config`
        8. topics: `curl -X PUT http://ip:port/connectors/<connector-name>/topics`
        9. list tasks: `curl -X GET http://ip:port/connectors/<connector-name>/tasks`
        10. get a task: `curl -X GET http://ip:port/connectors/<connector-name>/tasks/<task-id>`
        11. task status: `curl -X GET http://ip:port/connectors/<connector-name>/tasks/<task-id>/status`
    2. Common connectors
        1. Debezium for MySQL
            1. Configurations
                1. `name: <debm_mysql_connctor>`: connector name
                2. `connector.class: io.debezium.connector.mysql.MySqlConnector`
                3. `tasks.max: <num>`
                4. `database.hostname: <hpmaster>`
                5. `database.port: <3306>`
                6. `database.user: <ttl>`
                7. `database.password: <ttl>`
                8. `database.server.id: <1000>`
                    1. a random number between 5400 and 6400 is generated, which can be specified explicitly
                    2. this connector joins the MySQL database cluster as another server with this unique ID
                    3. must unique across all currently-running database processes in the MySQL cluster
                9. `database.server.name: <debm_mysql>`:  topic for consumer recording DDL operation
                10. `database.whitelist: <debm>`: use `database.include.list`
                11. `database.include.list: db1,db2`: a optional, comma-separated list of regular expressions
                12. `database.exclude.list: db1,db2`: do not set `database.include.list` and `database.exclude.list` together
                13. `table.include.list: tb1,tb2`
                14. `table.exclude.list: tb1,tb2`
                15. `column.include.list`
                16. `column.exclude.list`
                17. `database.history.kafka.topic: <his.debm_mysql>`: internal use only, should not used by consumers
                18. `database.history.kafka.bootstrap.servers: <hpmaster:9092>`
                19. `include.schema.changes: <true>`: monitor schema change, write to the topic `database.server.name`
                20. `include.query: <true>`: record sql, need `binlog_rows_query_log_events=ON` in mysql
                21. `snapshot.locking.mode`
                    1. `minimal`
                    2. `minimal_percona`
                    3. `extended`
                    4. `None`: avoid locking when get table meta data, but may cause inconsistence when an DDL cause schema changes
                22. `snapshot.mode`
                    1. `<default: initial>`
                    2. `when_needed`
                    3. `never`: need the binlog contains all the data about table including DDL statements, otherwise cause an error `Encountered change event for table some_db.some_table whose schema isn't known to this connector`
                    4. `schema_only`: capture the current schema of the database, and then start reading the binlog at the latest position(most recent change), **suit for merging with history data**
                    5. `schema_only_recovery`: use this to recovery when `schema_only` canse an error
            2. Topics
                1. `<database.history.kafka.topic>`: record DDL operations (SQL and binlog position)
                2. `<database.server.name>`: DDL topic
                3. `<database.server.name>.<databasname>.<tablename>`: DML topic name for every table created by connector
            3. Events
                1. `op`: DML operation
                    1. `c`: insert
                    2. `u`: update
                    3. `d`: delete
                2. `ddl`
                3. `databasename`
