# Kafka

1. References
    1. [Debezium Connector](https://debezium.io/releases/1.5/)
    2. [Kafka Connectors](https://www.confluent.io/product/connectors/)
    3. [Kafka Connector REST API](https://docs.confluent.io/3.0.0/connect/userguide.html)
    4. [Debezium for MySQL Properties](https://debezium.io/documentation/reference/connectors/mysql.html#mysql-connector-properties)
    5. [Understand Kafka as if you had designed it — Part 1](https://towardsdatascience.com/understanding-kafka-as-if-you-had-designed-it-part-1-3f9316cb8fd8)
    6. [Understanding Kafka as If You Had Designed It — Part 2](https://towardsdatascience.com/understanding-kafka-as-if-you-had-designed-it-part-2-33d0d24c79f1)
2. Concepts
    1. Broker: storage layer
    2. Connector: import and export data as event streams to integrate Kafka with existing systems
    3. Clients: talk to Kafka servers
    4. Event: also called record or message, record data happened
        1. Structure
            1. events: organized and durably stored in *topics*, similar to a folder in a filesystem, and the events are the files in that folder
            2. topics:
                1. partitioned, spread over a number of buckets located on different brokers
                2. new event published to a topic, is actually appended one of the topic's partition
                3. the same event key event are written to the same partition
                4. any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written
                5. every topic can be **replicated** at the level of topic partitions. a common production replication factor is 3
            3. events are not deleted after assumption, the retaining time can be defined through a per-topic configuraiton setting
        2. Contents
            1. key
            2. value
            3. timestamp
            4. headers: optional
3. Install
    1. [download](https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/)
    2. set `KAFKA_HOME=kafka_2.12-2.8.1` 
4. Service
    1. standalone
        1. start zookeeper: `bin/zookeeper-server-start.sh config/zookeeper.properties`
            1. `-daemon`
        2. stop zookeeper: `bin/zookeeper-server-stop.sh config/zookeeper.properties`
        3. start kafka: `bin/kafka-server-start.sh config/server.properties`
        4. stop kafka: `kafka-server-stop.sh <config/server.properties>`
    2. cluster
        1. start and stop every host
    3. topics
        1. list: `bin/kafka-topics.sh --list --bootstrap-server localhost:9092`
        2. describe: `bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092`
        3. create: `bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092`
            1. `--partitions <num>`
        4. delete: `bin/kafka-topics.sh --delete --topic quickstart-events --bootstrap-server localhost:9092`
    4. events
        1. produce: `bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092`
        2. consume: ` bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092`
        3. view group consume
            1. list groups: `kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list`
            2. one group: `kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group group1`
            3. all group: `kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups`
    5. delete local data and topics events: `rm -rf /tmp/kafka-logs /tmp/zookeeper`
    6.  connectors
        1. standalone: `connect-standalone.sh <config/connect-standalone.properties> <connector1.properties> [<connector2.properties]`
        2. cluster: `connect-distributed.sh [-daemon] <config/connect-distributed.properties>`
5. Configurations
    1. `config/server.properties`
        1. `broker.id=1`: the id of broker, must be unique integer for each broker
        2. `listeners=PLAINTEXT://host:9092`: listen port for client connecting, `[default: 9092]`
        3. `log.dirs=/path/1,/path/2`: where to store log files
        4. `zookeeper.connect=zoo1:2181,zoo2:2181...`: connect to zookeeper
        5. `num.partitions=2`: partitions per topic. more partitions allow greater parallelism for consumption, but result in more files across the brokers
    2. `config/connect-distributed.properties`
        1. `bootstrap.servers=host1:9092,host2:9092`: any available broker is enough
        2. `group.id=connect-cluster`: unique name for the cluster, used in forming the connect cluster group, must not conflict with consumer group IDs
        3. `key.converter=org.apache.kafka.connect.json.JsonConverter`: default
        4. `value.converter=org.apache.kafka.connect.json.JsonConverter`: default
        5. `key.converter.schemas.enable=false`: `[default: true]`
        6. `value.converter.schemas.enable=false`: `[default: true]`
        7. `offset.flush.interval.ms=10000`
        8. `rest.port=8083`: `[default: 8083]`
        9. `plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors`: set additional connector jar path
        10. config storage
            1. Notes
                1. `[default: connect-configs]`
                2. should be a single partition
                3. highly replicated, compacted, 3 or more, no more than broker numbers
            2. Settings
                1. `config.storage.topic=connect-configs`
                2. `config.storage.replication.factor=1`: for one broker
            3. Commands
                1. `kafka-topics.sh --create --topic connect-configs --bootstrap-server hpmaster:9092 --partitio --replication-factor 2 --config cleanup.policy=compact`
        11. offset storage
            1. Notes
                1. `[default: connect-offsets]`
                2. should be multiple partition
                3. replicated, compact, no more than broker numbers
            2. Settings
                1. `offset.storage.topic=connect-offsets`
                2. `offset.storage.replication.factor=1`: for one broker
                3. `offset.storage.partitions=25`
            3. Commands
                1. `kafka-topics.sh --create --topic connect-offsets --bootstrap-server hpmaster:9092 --partition --replication-factor 2 --config cleanup.policy=compact`
        12. status storage
            1. Notes
                1. `[default: connect-status]`
                2. should be multiple partition
                3. replicated, compact, no more than broker numbers
            2. Settings
                1. `status.storage.topic=connect-offsets`
                2. `status.storage.replication.factor=1`: for one broker
                3. `status.storage.partitions=5`
            3. Commands
                1. `kafka-topics.sh --create --topic connect-status --bootstrap-server hpmaster:9092 --partition --replication-factor 2 --config cleanup.policy=compact`
6. Connectors
    1. Command
        1. list connectors: `curl -X GET http://ip:port/connectors`: port `[default:8083]`
        2. crerate new connector: `curl -X POST -H "Content-Type:application/json" --data @mysql.properties.json http://hpmaster:8083/connectors`
        3. delete a connector: `curl -X DELETE http://ip:port/connectors/<connector-name>`
        4. restart a connector: `curl -X POST http://ip:port/connectors/<connector-name>/restart`
        5. status: `curl -X GET http://ip:port/connectors/<connector-name>/status`
        6. config: `curl -X GET http://ip:port/connectors/<connector-name>/config`
        7. config change: `curl -X PUT -H "Content-Type:application/json" http://ip:port/connectors/<connector-name>/config`
        8. topics: `curl -X PUT http://ip:port/connectors/<connector-name>/topics`
        9. list tasks: `curl -X GET http://ip:port/connectors/<connector-name>/tasks`
        10. get a task: `curl -X GET http://ip:port/connectors/<connector-name>/tasks/<task-id>`
        11. task status: `curl -X GET http://ip:port/connectors/<connector-name>/tasks/<task-id>/status`
    2. Common connectors
        1. Debezium for MySQL
            1. Configurations
                1. `name: <debm_mysql_connctor>`: connector name
                2. `connector.class: io.debezium.connector.mysql.MySqlConnector`
                3. `tasks.max: <num>`
                4. `database.hostname: <hpmaster>`
                5. `database.port: <3306>`
                6. `database.user: <ttl>`
                7. `database.password: <ttl>`
                8. `database.server.id: <1000>`
                    1. a random number between 5400 and 6400 is generated, which can be specified explicitly
                    2. this connector joins the MySQL database cluster as another server with this unique ID
                    3. must unique across all currently-running database processes in the MySQL cluster
                9. `database.server.name: <debm_mysql>`:  topic for consumer recording DDL operation
                10. `database.whitelist: <debm>`: use `database.include.list`
                11. `database.include.list: db1,db2`: a optional, comma-separated list of regular expressions
                12. `database.exclude.list: db1,db2`: do not set `database.include.list` and `database.exclude.list` together
                13. `table.include.list: tb1,tb2`
                14. `table.exclude.list: tb1,tb2`
                15. `column.include.list`
                16. `column.exclude.list`
                17. `database.history.kafka.topic: <his.debm_mysql>`: internal use only, should not used by consumers
                18. `database.history.kafka.bootstrap.servers: <hpmaster:9092>`
                19. `include.schema.changes: <true>`: monitor schema change, write to the topic `database.server.name`
                20. `include.query: <true>`: record sql, need `binlog_rows_query_log_events=ON` in mysql
                21. `snapshot.locking.mode`
                    1. `minimal`
                    2. `minimal_percona`
                    3. `extended`
                    4. `None`: avoid locking when get table meta data, but may cause inconsistence when an DDL cause schema changes
                22. `snapshot.mode`
                    1. `<default: initial>`
                    2. `when_needed`
                    3. `never`: need the binlog contains all the data about table including DDL statements, otherwise cause an error `Encountered change event for table some_db.some_table whose schema isn't known to this connector`
                    4. `schema_only`: capture the current schema of the database, and then start reading the binlog at the latest position(most recent change), **suit for merging with history data**
                    5. `schema_only_recovery`: use this to recovery when `schema_only` causes an error
            2. Topics
                1. `<database.history.kafka.topic>`: record DDL operations (SQL and binlog position)
                2. `<database.server.name>`: DDL topic
                3. `<database.server.name>.<databasname>.<tablename>`: DML topic name for every table created by connector
            3. Events
                1. `op`: DML operation
                    1. `c`: insert
                    2. `u`: update
                    3. `d`: delete
                2. `ddl`
                3. `databasename`
