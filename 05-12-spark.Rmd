# Spark

1. References
    1. [spark-notes](https://spoddutur.github.io/spark-notes/)
    2. [Structured Streaming + Kafka Integration Guide](http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
    3. [Structured Streaming Kafka Integration](https://docs.databricks.com/_static/notebooks/structured-streaming-etl-kafka.html)
    4. [Structured Streaming to PostgreSQL](https://3rdman.de/2019/08/spark-to-postgresql-real-time-data-processing-pipeline-part-5/)
    5. [Structured Streaming to Hbase](https://blog.csdn.net/yuyinghua0302/article/details/96574860)
2. Concepts
    1. driver
        1. run user's `main()` function
        2. execute parallel operations on a cluster
    2. ApplicationMaster
        1. negotiate resources from the ResourceManager
        2. work with NodeManager to execute
        3. monitor containers and resource their resources consumption
        4. when run on yarn, may need 1G memory and 1 Executor
    3. Task: smallest unit of execution working on a partition of data
    4. Executor: slots where tasks run as threads
        1. vcores: number of tasks (threads) running within the executor
        2. memory: cached data within the executor
            1. full memory requested to yarn per executor = `spark.executor.memory` + `spark.yarn.executor.memoryOverhead`
            2. `spark.yarn.executor.memoryOverhead` = `Max(384MB, 7% of spark.executor-memory)`
    5. MemoryOverhead
3. Using Scala
    1. Dependencies
        1. `"org.apache.spark" %% "spark-sql" % "3.1.2" % "provided"`
    2. Initialization
        1. `import org.apache.spark.sql.SparkSession`
        2. `val spark = SparkSession.builder().appName("spark example").config("spark.config.opiton", "config-value").getOrCreate()`
    3. Configurations
        1. `spark.conf.get("spark.config.option")`
        2. `spark.conf.set("spark.config.option", "other-config-value")`
        3. `import spark.implicits._`
            1. implicitly convert RDDs to DataFrames or use `$-notation`
            2. provide Encoder for most common types automatically
    4. Stop
        1. `spark.stop()`
    5. Application
        1. define a `main()` method instead of extending `scala.App`. Subclasses of `scala.App` may not work correctly

    ```scala
    import org.apache.spark.sql.SparkSession
    
    object SimpleApp {
        def main(args: Array[String]) {
            val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
            // this import is needed to use the $-notation
            import spark.implicits._
            spark.read.textFile(logFile)
            ...
            spark.stop()
        }
    }
    ```
4. Data Structure
    1. Prepare

        ```scala
        case class Person(name: String, age: Long)

        val peopleRDD = spark.sparkContext.textFile("people.txt")
        val rowRDD = peopleRDD.map(_.split(",")).map(attrs => Row(attrs(0), attrs(1).trim.toInt))

        val schemaString = "name age"
        val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, nullable = true))
        val schema = StructType(fields)

        val peopleDF = spark.createDataFrame(rowRDD, schema)
        ```
    2. `org.apache.spark.sql.types`
        1. Numeric
            1. Types
                1. `ByteType`
                2. `ShortType`
                3. `IntegerType`
                4. `LongType`
                5. `FloatType`
                6. `DoubleType`
                7. `DecimalType`: backed internally by `java.math.BigDecimal`
            2. Special Values
                1. `Inf, +Inf, Infinity, +Infinity`: positive infinity
                    1. `FloatType`: equivalent to Scala `Float.PositiveInfinity`
                    2. `DoubleType`: equivalent to Scala `Double.PositiveInfinity`
                2. `-Inf, -Infinity`: negative infinity
                    1. `FloatType`: equivalent to Scala `Float.NegativeInfinity`
                    2. `DoubleType`: equivalent to Scala `Double.NegativeInfinity`
                3. `NaN`: not a number
                    1. `FloatType`: equivalent to Scala `Float.NaN`
                    2. `DoubleType`: equivalent to Scala `Double.NaN`
        2. String
            1. `StringType`
            2. `Varchar(length)`
                1. a variant of `StringTpe` which has a length limitation
                2. data writing will fail if the input string exceeds the length
                3. only be used in table schema, not functions or operations
            3. `CharType`
                1. a variant of `StringTpe` which has a fixed length
                2. reading column of type `CharType(n)` always returns string values of length `n`
                3. Char type column comparison will pad the short one to the longer length
        3. Binary
            1. `BinaryType`: represent byte sequence values
        4. Boolean
            1. `BooleanType`
        5. datetime
            1. `TimestampType`: represent an absolute point in time with fields year, month, day, hour, minute, and second, with the session local time-zone
            2. `DateType`: represent values with fields year, month and day, without a time-zone
        6. Complex
            1. `ArrayType(elementType, containsNull: [default: true])`: a sequence of elements with type `elementType`
                1. `containsNull`: indicate if elements can have null values
            2. `MapType(keyType, valueType, valueContainsNull: [default: true])`: a set of key-value pairs
                1. keys are not allowed to have null values
                2. `valueContainsNull`: indicate if MapType values can have null values
            3. `StructType(fields)`: a sequence of `StructFields(fields)`, two fields with the same name are not allowed
                1. `org.apache.spark.sql.types.StructField(name, dataType, nullable: [default: true])`
                2. Methods
                    1. `val schema = StructType(field1, field2, ...)`
                    2. `val schema = new StructType().add("name", StringType)...`
    3. `org.apache.spark.rdd.RDD`
        1. Properties
            1. an immutable distributed collection of JVM objects partitioned across nodes
            2. strong-typed and operate in parallel with low-level API transformations and actions
        2. Structure
            1. Descriptions
                1. record data information, only load them when take *action* functions
                2. serve like a representation of a DAG of instructions, telling how to retrieve data and how to do with it, similar to a SQL execution plan
            2. Members
                1. a list of partitions to get data partitions
                2. a function for computing each split
                3. a list of dependencies on other RDDs
                4. optionally, a Partitioner for key-value RDDs
                5. optionally, a list of preferred locations to compute each split on (support data localization)
        3. Initialization
            1. From Driver
                1. `val data = Array(1, 2, 3, 4, 5)`
                2. `val peopleRDD = spark.sparkContext.makeRDD(data)`
                3. `val peopleRDD = spark.sparkContext.parallelize(data)`
            2. From File: `sparkContext` is used to create RDD
                1. `val peopleRDD = spark.sparkContext.textFile("people.txt")`
            3. From DataSet
                1. `val peopleRDD = peopleDS.rdd`
            4. From DataFrame
                1. `val peopleRDD = peopleDF.rdd`
        4. Properties
            1. `getNumPartitions`
        5. Transforms: lazy, create a new dataset from an existing one, do not compute their results right away
            1. Value
                1. `.map()`: `df.map(d => d.getAs[Long]("age"))`
                2. `.flatMap()`
                3. `.filter()`
                4. `.distinct()`
                5. `.union()`
            2. Key Value
                1. `.reduceByKey()`
                2. `.groupByKey()`
                3. `.sortByKey()`
        6. Actions: return a value to the driver program after running a computation on the dataset
            1. `.collect()`: fetch entire RDD to the driver program
            2. `.take([n])`: fetch first `n` number of elements, safer then `.collect()` when dataset is large
            3. `.count()`
            4. `.first()`
            5. `.reduce()`
            6. `.foreach()`
            7. `.countByKey()`
        7.  Cache: persist an RDD datasets into a cluster-wide in-memory cache
            1. `.persist()`
            2. `.cache()`: a shorthand for using the default storage level `StorageLevel.MEMORY_ONLY`
        8.  Shared Variable
            1. Broadcast: keep a read-only variable cached on each machine rather than shipping a copy of it with tasks
                1. `val v = sc.broadcast(Array(1, 2, 3))`
                2. `v.value`
                3. `v.unpersist()`: release resources that the broadcast variable copied onto executors. it will be re-broadcast when needed
                4. `v.destroy()`: permanently release all resources used by the broadcast variable which can’t be used after that
            2. Accumulator
                1. Properties
                    1. only added to through an associative and commutative operation
                    2. used to implement counters or sums
                    3. natively supports accumulators of numeric types
                    4. tasks running on a cluster can add to it using `add()` method, but they cannot read its value.
                    5. only the driver program can read the accumulator's value, using `value` method
                    6. performed inside **actions only**, Spark guarantees that each task’s update to the accumulator will only be applied once, restarted tasks will not update the value. In transformations, each task's update may be applied more than once if re-executed
                2. type
                    1. `sc.longAccumulator("my accumulator')`
                    2. `sc.DoubleAccumulator("my accumulator')`
                3. Examples
                    1. `val acc = sc.longAccumulator("My Accumulator")`
                    2. `sc.parallelize(Array(1, 2, 3, 4)).foreach(x => acc.add(x))`
                    3. `acc.value`
    4. `org.apache.spark.sql.Dateset`
        1. Properties
            1. a collection of compile-time safety and strong-typed JVM objects dictated by a case class
            2. APIs are all expressed as lambda functions
            3. `Dataset[T]` typed API is optimized for data engineering tasks 
        2. Initialization
            1. From Driver
                1. `val peopleDS = Seq(Person("Andy", 32)).toDS()`
                2. `val peopleDS = spark.createDateset(List(Person("Andy", 32), Person("Bob", 12)))`
            2. From Spark
                1. `spark.range(100)`
            3. From RDD
                1. `val peopleDS = peopleRDD.toDS()`
            4. From DataFrame
                1. `val peopleDS = peopleDF.as[Person]`
        3. Metadata
            1. `.columns`
            2. `.schema`
            3. `.printSchema`
            4. `.explain([true, false])`: `[default: false]`
        4. Type-Safe
            1. Transforms
                1. `.map(d => (d.name, d.age == 10))`
                2. `.flatMap()`
                3. `.filter(d => d.age === 10)`
                4. `groupByKey()`
            2. Actions
                1. `.count()`
                2. `.collect()`
                3. `.take(10)`
                4. `.show()`: reformat data
                    1. `show()`
                    2. `show(false)`
                    3. `show(10)`
                    4. `show(10, false)`
                5. `.first()`
                6. `.reduce()`
        5. Untyped (aka DataFrame operations): `import spark.implicits._` to use `$-notation`
            1. `.select("name", "age")`
            2. `.select($"name", $"age" + 1, $"age" > 20)`
            3. `.filter($"age" === 10)`
            4. `.where("sigal > 10")`
            5. `.groupBy("age", "name").count()`
    5. `org.apache.spark.sql.Row`: a generic untyped JVM object
    6. `org.apache.spark.sql.DataFrame`
        1. Properties
            1. an alias for a collection of generic objects of `Dataset[Row]` and organized into named columns
            2. faster and suitable for interactive analysis
        2. Initialization
            1. From Driver
                1. `val PeopleDF = List(("Andy", 32)).toDF("name", "age")`
                2. `val PeopleDF = spark.createDataFrame(List(Person("Andy", 32), Person("Bob", 12)))`
            2. From Files
                1. `val peopleDF = spark.read.json("people.json")`
                2. `val userDF = spark.read.format("json").load("users.json")`
                3. `val userDF = spark.read.load("users.parquet")`
                4. `val userDF = spark.sql("select * from parquet.users.parquet")`
            3. From RDD
                1. `val peopelDF = peopleRDD.toDF`
                2. `val peopelDF = spark.createDataFrame(rowRDD, schema)`
        3. SQL
            1. `df.createOrReplaceTempView("people"); spark.sql("select * from people")`: session scoped
            2. `df.createOrReplaceGlobalTempView("people"); spark.sql("select * from global_temp.people")`: application scoped
            3. `spark.newSession().sql("select * from global_temp.people")`
5. Sources
    1. Methods
        1. Load
            1. `val peopleDF = spark.read.text("people.txt")`
            2. `val peopleDF = spark.read.csv("people.csv")`
            3. `val peopleDF = spark.read.parquet("people.parquet")`
            4. `val peopleDF = spark.read.json("people.json")`
            5.  `val peopleDF = spark.read.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)`
            6. `val peopleDF = spark.read.format("csv").option("sep", ";").option("header", "true").option("inferSchema", "true").load("people.csv")`
            7. `val peopleDF = spark.read.format("json").load("people.json")`
            8. `val peopleDF = spark.read.format("parquet").load("dir1/", "dir2/")`
            9.  `val peopleDF = spark.read.format("jdbc").option(...).load()`
            10. `val peopleDF = spark.read.load("people.parquet")`
            11. ``val sqlDF =spark.sql("select * from parquet.`people.parquet`")``
        2. Save
            1. `peopleDF.write.parquet("people.parquet")`
            2. `peopleDF.write.parquet("data/people/key=1")`
            3. `peopleDF.write.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)`
            4. `peopleDF.write.format("csv").option("header", "true").mode(modeValue).save("users.csv")`
            5. `peopleDF.write.format("jdbc").option(...).save()`
            6. `peopleDF.write.save("people.parquet")`
            7. `peopleDF.write.partitionBy("color").bucketBy(10, "name").sortBy("age").saveAsTable("t_par_but")`: save to table
            8. `peopleDF.write.partitionBy("key").format("hive").saveAsTable("hive_part_tbl")`
            9. `peopleDF.write.format("orc").option("orc.bloom.filter.columns", "favorite_color").option("orc.dictionary.key.threshold","1.0").option("orc.column.encoding.direct", "name").save("users_with_options.orc")`
    2. Configurations
        1. `org.apache.spark.sql.SaveMode`
            1. `[default: SaveMode.ErrorIfExists]`
            2. `SaveMode.Append`
            3. `SaveMode.Overwrite`
            4. `SaveMode.Ignore`: do nothing when exists, similar to `create table if not exists` in SQL
        2. `spark.sql.source.default: [default: parquet]`
            1.  Files
                1. All
                    1. Paths
                        1. local file: `file:///home/...`
                        2. hdfs: `hdfs://..`
                    2. Load Options
                        1. `spark.sql("set spark.sql.files.ignoreCorruptFiles=true")`: enable ignore corrupt files
                        2. `spark.sql("set spark.sql.files.ignoreMissingFiles=true")`: ignore deleted file after construct the DataFrame
                        3. `spark.sql("spark.sql.sources.partitionColumnTypeInference.enabled=false")`
                        4. `spark.sql("spark.sql.parquet.mergeSchema=true")`
                        5. `.option("pathGlobFilter", "*.parquet")`
                        6. `.option("recursiveFileLookup", "true")`: when `true`, recursively load files and disable partition inferring
                        7. Modification Time Path Filter: not support Structured Streaming file sources
                            1. `.option("modifiedBefore", "2020-07-01T05:30:00")`
                            2. `.option("modifiedAfter", "2020-06-01T05:30:00")`
                    3. Save Options
                        1. `.partitionBy("favorite_color")`
                        2. `.bucketBy(42, "name")`
                        3. `.sortBy("age")`
                        4. `.mode(saveMode)`
                2. Formats
                    1. `[default: parquet]`: short name for built source `org.apache.spark.sql.parquet`
                        1. Load Options
                            1. `.option("mergeSchema", "true")`
                        2. Save Options
                            1. `.parquet("data/test_table/key=1")`
                    2. `orc`
                        1. Save Options
                            1. `option("orc.bloom.filter.columns", "favorite_color")`
                            2. `option("orc.dictionary.key.threshold", "1.0")`
                            3. `option("orc.column.encoding.direct", "name")`
                    3. `json`
                        1. Load Options
                            1. `.option("multiline", "true")`
                    4. `avro`
                    5. `csv`
                        1. All Options
                            1. `.option("sep", ",")`
                            2. `.option("header", "true")`
                        2. Load Options
                            1. `.option("inferSchema, "true")`
                            2. `.option("pathGlobalFilter", "*.csv")`
                    6. `text`
            2. `hive`
                1. Access:
                    1. client `${HIVE_CONF_DIR}/hive-site.xml`
                        1. `hive.metastore.uris: thrift://hpmaster:9083`
                    2. `val spark = SparkSession.builder().enableHiveSupport().getOrCreate()`
                    3. `spark.sql(sql)`
                2. Write Options
                    1. `spark.sqlContext.setConf("hive.exec.dynamic.partition", "true")`
                    2. `spark.sqlContext.setConf("hive.exec.dynamic.partition.mode", "nonstrict")`
                3. Save Options
                    1. `.saveAsTable("tbl_name")`
            3. `jdbc`
                1. All Options
                    1. `.option("url", "jdbc:postgresql://host:port/dbname?user=u&password=p")`
                    2. `.option("user", "username")`
                    3. `.option("password", "password")`
                    4. `.option("dbtable", "schema.tablename")`: allow subquery in parentheses
                    5. `.option("driver","org.postgresql.Driver")`: JDBC database driver, when `spark-shell` or `spark-submit` speicify `--driver-class-path`, no need to config this option
                    6. `.option("numPartitions", 10)`
                    7. `.jdbc("url", "schema.table", connectionProperties)`

                        ```scala
                        val connectionProperties = new java.util.Properties()
                        connectionProperties.put("user", "username")
                        connectionProperties.put("password", "password")
                        connectionProperties.put("driver", "com.mysql.cj.jdbc.Driver")
                        ```
                    8. `.explain([true, false])`: execute plan
                2. Load Options
                    1. `option("query", "select * from t1")`: can not use with `dbtable` at the same time
                    2. `option("partitionColumn", "column"); option("upperBound", 1000); option("lowerBound", 10)`: stride size `s = (upperBound-lowerBound)/numPartitions`, the range is `[[, s), [s, 2s), ...]`
                    3. `option("fetchsize", [default: 1000])`
                    4. `option("customSchema", "id DECIMAL(38, 0), name STRING")`
                3.  Save Options
                    1. `option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")`
                    2. `option("batchsize", 1000)`
                    3. `option("truncate", true)`: `[default: false]`, when `SaveMode.Overwrite` enabled, spark just truncate table, not drop
                    4. `option("isolationLevel"])`
                        1. `None`
                        2. `READ_UNCOMMITED`
                        3. `[default: READ_UNCOMMITED]`
                        4. `REPEATABLE_READ`
                        5. `SERIALIZABLE`
            4.  `libsvm`
6. Structured Streaming
    1. All Options
        1. `.option("checkpointLocation", "/path/to/HDFS/dir")`
    2. Kafka
        1. Dependencies
            1. `"org.apache.spark" % "spark-sql-kafka-0-10_2.12" % "3.1.2"`
        2. Deployment
            1. `--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2`
        3. All Options
                1. `.option("kafka.bootstrap.servers", "host1:9092,host2:port2...")`: comma separated list
                2. `[.option("includeHeaders", "{[default: false], true})]`: whether to include the Kafka headers in the row
        4. Load
            1. Options
                1. `.option("subscribe", "topic")`
                    1. `topic1`: one topic
                    2. `topic1,topic2`: multiple topics
                    3. `topic.*`: a pattern, java regex string
                2. `[.option("startingOffsets", "latest")]`
                    1. `earliest`: default for batch, read all data available in the Kafka
                    2. `latest`: only for streaming and is default value, not allowed for batch query, read only new data that's not been processed
                    3. `"""{"topic1": {"0":23, "1": -1"}, "topic2":{"0":-2}}"""`: json string specifying a starting offset for a topic partition
                        1. `-2`: as an offset to refer to `earliest`
                        2. `-1`: to `lastest`, not allowed for batch query
            2. Streaming: `val df = spark.readStream.format("kafka").load()`
                1. Schema
                    1. `key: binary`
                    2. `value: binary`
                    3. `topic: string`
                    4. `partition: integer`
                    5. `offset: long`
                    6. `timestamp: timestamp`
                    7. `timestampType: integer`
                2. Processing
                    1. `df.selectExpr("CAST(key as STRING) as id", "CAST(value as STRING)")`: convert binary data to String
                    2. `var parseddf.select(from_json(col("value").cast("string"), schema).alias("parsed_value"))`: parse json string
            3. Batch: use `read` instead of `readStream`, `val df = spark.read.format("kafka").load()`
                1. Options
                    1. `[.option("endingOffsets", "latest")]`
                        1. `[default: latest]`
                        2. `"""{"topic1": {"0":23, "1": -1"}, "topic2":{"0":-1}}"""`: json string specifying an ending offset for each topic partition
                            1. `-1`: as an offset to refer to `latest`
                            2. ~~`-2`: to `earliest`, not allowed~~
        5. Write
            1. Options
                1. `[.option("topic", "topic1")]`: `[default: none]`, override any topic column existing in the data
            2. Streaming
                1. `val ds = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").writeStream.format("kafka").option("topic", "topic1").start()`: specify a topic in an option
                2. `val ds = df.selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)").writeStream.format("kafka").start()`: specify a topic in the data
            3. Batch: use `write` instead of `writeStream` 
                1. `val ds = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").write.format("kafka").option("topic", "topic1").save()`: specify a topic in an option
                2. `val ds = df.selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)").write.format("kafka").save()`: specify a topic in the data
    3. Files
        1. Write: `df.writeStream.format("json").start()`
            1. Options
                1. `.format()`
                2. `.outputMode("append")`
                    1. `[default: append]`
                    2. `complete`
                    3. `update`
                3. `.option("path", "path")`
    5. Other Sinks
        1. extend class: `org.apache.spark.sql.ForeachWriter`
        2. override functions
            1. `open(partitionId: Long, epochId: Long): Boolean`
            2. `process(value: T): Unit`
            3. `close(errorOrNull: Throwable): Unit`
7. `org.apache.spark.sql.functions`
    1. Array and Map
        1. `size()`
        2. `explode()`
        3. `explode_outer()`
    2. Array
        1. `array_contains()`
        2. `arrays_zip()`
        3. `element_at()`
        4. `reverse()`
        5. `flatten()`
        6. `shuffle()`
        7. `slice()`
    3. String
        1. `split()`
    4. Json
        1. `get_json_object(e: Column, path: String)`
            1. path: `$.key1.key2...`
        2. `json_tuple`
        3. `from_json()`
        4. `to_json()`
