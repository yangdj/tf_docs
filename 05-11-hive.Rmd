# Hive

1. References
    1. [hive data types](https://understandingbigdata.com/hive-data-types)
    2. [LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)
2. Installation
    1. configuration
        1. `hive.metastore.warehouse.dir`:  where to store internal table data
3. Settings
    1. `SET hive.default.fileformat=orc`
    2. `SET hive.mapred.mode=[nonstrict, strict]`:  when `strict`, SQL must add `limit` when using `order by`
    3. `SET hive.strict.checks.cartesian.product=false`: enable cartesian join
    4. `SET hive.execution.engine={mr,spark,tez}`: set execution engine
    5. `SET hive.tez.container.size=6144`: set tez memory size
    6. `SET mapred.reduce.tasks=60`: set numbers of reducers
    7. `SET mapred.job.queue.name=queue0`
    8. `SET tez.queue.name=queue0`
    9. `SET hive.auto.convert.join=true`: join in mapper by load small table into memory when big table joins small table, this reduces shuffle operations
    10. `SET hive.groupby.skewindata=true`: enable load balance when data skews, default false
    11. Compress
        1.  `SET hive.exec.compress.output=true`: compress output
        2.  `SET io.seqfile.compression.type=BLOCK [BLOCK,RECORD,NONE]` 
            1. `RECORD`: compress each record individually
            2. `BLOCK`: recommended, buffer up 1MB(default) before doing compression
        3. `SET io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec`
        4. `SET mapred.output.compress=true;`
        5. `SET mapreduce.output.fileoutputformat.compress=true`
        6. `SET mapred.output.compression.type=BLOCK`
        7. `SET mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec`
        8. `SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzoCodec`
    12. Partition
        1. `SET hive.exec.dynamic.partition=true`: enable dynamic partition
        2. `SET hive.exec.dynamic.partition.mode=nostrick`: support all partitions dynamically, otherwise at least one should be static
        3. `SET hive.exec.max.dynamic.partitions=100000`: max number for dynamic partitions in one SQL, default 1000
        4. `SET hive.exec.max.dynamic.partitions.pernode=100000`: max number partitions support by one mapper or reducer, default 100
        5. `SET hive.exec.max.created.files=100000`: max created file number for mapper and reducer in one job,default 100000
        6. `SET hive.error.on.empty.partition=false`: whether allowing empty results, when `true` throw an error
4. Data Type
    1. Primitive
        1. Integers 
            1. tinyint: 1-byte signed, -2^7 ~ 2^7 - 1, `100Y`
            2. smallint: 2-byte signed, -2^15 ~ 2^15 - 1, `100S`
            3. int / integer: 4-byte signed, -2^31 ~ 2^21 - 1, default for integer literal
            4. bigint: 8-byte signed, -2^63 ~ 2^63 - 1, `100L`
                1. integer larger than bigint must be handled with `decimal(38, 0)` with postfix BD, `100BD`
        2. Floating
            1. float: 4-byte single precision
            2. double: 8-byte double precision, default for floating literal
            3. double precision: alias for double, only available from Hive 2.2.0
            4. decimal(precision, scale):
                1. default `decimal(10, 0)`
                2. `precision` <=38, and should >= `scale`
                3. precision is the total width except the point; scale is width after the point
                4. if the integer part width `(precision - scale)` less than actual integer part width, `NULL` will be return
            5. numeric: same as decimal from Hive 3.0
        3. String
            1. char: `char(10)`, fix length, no more than 255, spaces may be padded
            2. varchar: `varchar(10)`, variable length, no more than 65535, no more than given length
            3. string: recommended, could be 2GB size
        4. boolean: `[true, false]`
        5. Date & Time
            1. timestamp: stored as an offset from the unix epoch
                1. integer input interpreted as unix timestamp in seconds, but in `cast`, interpreted as **mileseconds not seconds**
                2. floating input interpreted as unix timestamp in seconds
                3. string input follow JDBC compliant `java.sql.Timestamp format` format `yyyy-mm-dd hh:mm:ss[.f{0,9}]` with optional nanoseconds
            2. date: a particular year/month/day value in the `yyyy-mm-dd` format, without a time of day component, range `0000-01-01 ~ 9999-12-31`
            3. interval: time units, `second(s)/minute(s)/hour(s)/day(s)/month(s)/year(s)`
                1. `interval '1' day`
                2. `interval '1-2' year to month` 
        6. binary: array of bytes
    2. Complex: collection type
        1. array: collection of similar data type
            1. `column[index]`: index starts from 0
            2. `score array<string>`:  specify column data type
            3. `collection items terminated by '$'`: specify elements separator when creating table
        2. struct: collection of named fields where each field can be of any primitive type
            1. `column.fieldname`
            2. `gamescore struct<game_name:string, score:int>`: specify column type and fields name and type
            3. `collection items terminated by '$'`: specify element separator
        3. map: collection of key value pairs
            1. `column[keys]`
            2. `score map<string, int>`
            3. `collection items terminated by '$'`: specify element separator
            4. `map keys terminated by ':'`: specify key separator
       1. NULL: for missing value, `LazySimpleSerDe` interprets the string `\N`as `NULL` when importing
5. Object Operations
    1. database
        1. `USE dbname`: switch to database dbname
        2. `CREATE DATABASE db_name [LOCATION 'oss://bilog/hivedb/db_name.db']`
        3. `DROP DATABASE [IF EXISTS] dbname`
    2. table
        1. `CREATE TABLE`
            1. `SHOW CREATE TABLE [db_name.]table_name`: show table creation statement
            2. `SHOW TALBE LIKE '*tbl_name*'`: support wildchar
            3. internal table
                1. store data in `hive.metastore.warehourse.dir(/user/hive/warehouse/databasename.db)`
                2. on `DROP`, Hive deletes both the metadata and actual data 
                3. `TRUNCATE` works, but partitions remain as it is
            4. external table
                1. store data outside of Hive
                2. on `DROP`, Hive only deletes metadata, but the actual data stays as it is
                3. `TRUNCATE` does not work
                4. `SHOW CREATE TABLE` tells whether the table is internal or external. If find keyword `external` then external, else internal
                5. when creating external table without `LOCATION ...`, the table will be created like internal table
            5. ROW FORMAT
                1. `DELIMITED`: textfile format, specify delimiter, escape character, null character and so on
                2. `SerDe`: specify class to split rows into columns and serialize columns to rows, may need load JAR file using `ADD JAR ***.jar`
                    1. `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`: textfile, default SerDe
                            1. SERPROPERTIES
                                1. `'field.delim'='\b'`: fields separator, use unicode `\u0008`
                                2. `'colelction.delim'='$'`: colletion separator
                                3. `'mapkey.delim'=':'`: map key separator
                                4. `'line.delim'='\n'`: line separator
                                5. `'serialization.null.format'='\\N'`: NULL identifier, `\N` default for `LazySimpleSerDe`
                                6. `'serialization.format'='\b'`: fields separator for serialization
                    2. 
            6. STORE AS: file format for the table
                1. system format

                    <div id="hive-format">
                    : (\#tab:hive-format) Hive System format

                    | Format       | SerDe                                                            | InputFormat                                                     | OutputFormat                                                     |
                    | ------------ | ---------------------------------------------------------------- | --------------------------------------------------------------- | ---------------------------------------------------------------- |
                    | textfile     | `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`             | `org.apache.hadoop.mapred.TextInputFormat`                      | `org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat`     |
                    | sequencefile | `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`             | `org.apache.hadoop.mapred.SequenceFileInputFormat`              | `org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat`      |
                    | rcfile       | `org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe` | `org.apache.hadoop.hive.ql.io.RCFileInputFormat`                | `org.apache.hadoop.hive.ql.io.RCFileOutputFormat`                |
                    | orc          | `org.apache.hadoop.hive.ql.io.orc.OrcSerde`                      | `org.apache.hadoop.hive.ql.io.orc.OrcInputFormat`               | `org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat`               |
                    | parquet      | `org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe`    | `org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat` | `org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat` |
                    | avro         | `org.apache.hadoop.hive.serde2.avro.AvroSerDe`                   | `org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat`    | `org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat`    |
                    
                    </div>

                2. custom format specifies `INPUTFORMAT` and `OUTPUTFORMAT` class, and choose `ROW FORMAT SERDE`
            7. TBLPROPERTIES:
                1. General
                    1. `'EXTERNAL'='false'`: make table internal
                    2. `'auto.purge'='ture'`: remove data directly, not moved to trash firstly
                    3. `'immutable'='true'`: from 0.13.0, default `false`, allow the first insert data
                2. orc
                    1. `orc.compress=[ZLIB, SNAPPY, NONE]`: default ZLIB
                    2. `orc.compress.size=262144`: bytes in each compression chunk
                    3. `orc.strip.size=268435456`: bytes in each stripe
                    4. `orc.row.index.stride=10000`: number of rows between index entries, must be >= 1000
                    5. `orc.create.index=true`: whether to create row indexe    s

            ```sql
            CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
                (col_name data_type [COMMENT 'col_comment'], ...)
                [PARTITIONED BY (col_name data_type [COMMENT 'col_comment..'], ...)]
                [COMMENT 'table_comment..']
                [ROW FORMAT 
                    (SERDE 'serde_class' [with SERDEPROPERTIES('key1'='value1', ...)])
                    | (DELIMETED
                        [FIELDS TERMINATED BY field_separator_char]
                        [ESCAPED BY escape_char]
                        [COLLECTION ITEMS TERMINATED BY collection_separator_char]
                        [MAP KEYS TERMINATED BY map_key_separator_char]
                        [LINES TERMINATED row_separator_char]
                        [NULL DEFINED AS null_char]
                    )
                ]
                [STORED AS (file_format | INPUTFORMAT 'inputformat_class' OUTPUTFORMAT 'outputformat_class')]
                [LOCATION 'file_path']
                [TBLPROPERTIES ('key1'='value1', 'key2'='value2', ...)]
            ```

        2. `INSERT`
            1. `INSERT INTO foo VALUES (...), (...), ...`
            2. `INSERT INTO foo SELECT ...`
            3. `insert into foo PARTITION(dt) ...`
            4. `INSERT OVERWRITE TABLE foo VALUES (), () ...` 
            5. `INSERT OVERWRITE TABLE foo SELECT ...` 
        3. `ALTER TABLE`
            1. `ALTER TABLE table_name RENAME TO new_table_name`: rename table
            2. `ALTER TABLE table_name SET TBLPROPERTIES('key'='value', ...)`: alter table properties, add or modify, not able to remove
            3. `ALTER TABLE table_name SET FILEFORMAT file_type`: alter storage properties, see the previous system format
            4. SerDe
                1. `ALTER TABLE table_name [partition(...)] SET SERDE 'serde_class_name' WITH SERDEPROPERTIES ('prob1'="value1", ...)`: set serde class and properties
                2. `ALTER TABLE table_name [partition(...)] SET SERDE 'serde_class_name'`: set SerDe class
                3. `ALTER TABLE table_name [partition(...)] SET SERDEPROPERTIES ('prob1'="value1", ...)`: set SerDe properties
        4. `LOAD`
            1. `LOAD DATA [LOCAL] INPATH 'file_path' [OVERWRITE] INTO TABLE foo [PARTITION(year='2021', month='06')]`
            2. `LOAD DATA [LOCAL] INPATH 'file_path' [OVERWRITE] INTO TABLE foo [PARTITION(year='2021', month='06')] [INPUTFORMAT 'input_format' SERDE 'serde']` (3.0 or later)
            3. INPATH: path can be file or a directory, when directory, all files will be moved
                1. `project/data1`: relative path
                2. `/user/hive/project/data1`: absolute path
                3. `file:///user/hive/project/data1`: complete file path point to local file
                4. `hdfs://namenode:9000/user/hive/project/data1`: URI with schema and optional authorized info
        5. `DROP`
            1. `DROP TABLE [IF EXISTS] foo`: data moved to trash folder first
            2. `DROP TABLE [IF EXISTS] foo PURGE`: data not moved to trash, but removed permanently, make sure the data is not needed
            3. `ALTER TABLE foo SET TBLPROPERTIES('EXTERNAL'='false'); DROP TABLE foo`: to drop external table data, make the table internal first, then drop table

    3. column
        1. `DESC table_name`: show columns info, names ,data_type and comment
        2. `DESC EXTENDED table_name`: list full info
        3. `DESC EXTENDED table_name PARTITION(ds='2021-06-18')`: list specified partition
        4. `ALTER TABLE foo ADD COLUMNS (col_name col_datatype, ...) [CASCADE|RESTRICT]`: add columns at the end of existing columns, if the table is partitioned, the columns get added at the end but before the partitioned column 
        5. `ALTER TABLE foo CHANGE [COLUMN] col_name new_column_name col_datatype [COMMENT col_comment] [FIRST|AFTER col] [CASCADE|RESTRICT]`
        6. `ALTER TABLE foo REPLACE COLUMNS (col_name datatype, ...) [CASCADE|RESTRICT]`: remove all columns and add new columns, the data is not dropped
        7. `CASCADE` and `RESTRICT` are for alter columns in partitioned table
            1. `CASCADE`: changes are propagated to all the existing and future partitions
            2. `RESTRICT`: changes are visible for new partitions created, existing partitions are not impacted

    4. partition
        1. `SHOW PARTITIONS foo`: show all partitions
        2. `ADD`: `ALTER TABLE foo ADD IF NOT EXISTS PARTITION(year='2021', month='06', day='08') [location ...]`
        3. `RENAME`: `ALTER TABLE foo PARTITION(year='2012') rename to PARTITION(year='2021')`
        4. `DROP`: `ALTER TABLE foo DROP [IF EXISTS] PARTITION(year='2021', month='06', day='08') [location ...]`
        5. `CHANGE`
            1. alter column in all partition : `ALTER TABLE foo PARTITION(ds, hr) CHANGE COLUMN dec_column_name dec_column_name decimal(38,18)`
            2. alter column in specified partition column: `ALTER TABLE foo PARTITION(ds='2021-06-08', hr=12) CHANGE COLUMN dec_column_name dec_column_name decimal(38,18)`
        6. `INSERT`
            1. all dynamic: 
                1. `INSERT INTO foo partition(year, month, day) select ...`
                2. `INSERT OVERWRITE TABLE foo PARTITION(year, month, day) SELECT ..., t.year, t.month, t.day from ...`
            2. partial dynamic:
                1. `INSERT INTO foo PARTITION(year='2021', month='06', day) SELECT ...`
                2. `INSERT OVERWRITE TABLE foo PARTITION(year='2021', month='06', day) SELECT ...`
    5. buckets(cluster): data in each partition divided into buckets based on the value of a hash function of some column of the table

6. SQL
    1. Operators 
        1. Precedenses
            1. `[], .`: element selector, dot
            2. `+, -, ~`: unary prefix
            3. `IS NULL, IS NOT NULL`: unary suffix
            4. `^`: bitwise xor
            5. `*, /, %, DIV`: multiply, float divide, mod, integer divide
            6. `+, -`: plus and minus
            7. `||`: string concatenation
            8. `&`: bitwise and
            9. `|`: bitwise or
        2. Relational
            1. `=, ==, <>, !=, <, <=, >, >=`
            2. `A [NOT] BETWEEN B AND C`
            3. `A IS [NOT] NULL`
            4. `A [NOT] LIKE B`
            5. `A RLIKE B`
            6. `A REGEXP B`: same as `A RLIKE B`
        3. Arithmetic
            1. `+, -, *, /, DIV, %`
            2. `&, |, ^, ~`
        4. Logical
            1. `AND, OR, NOT, !`
            2. `IN, NOT IN`
        5. String
            1. `||`: short for `concat(A, B)`
        6. Complex

            | Type   | Constructor           | Operator |
            | ------ | --------------------- | -------- |
            | array  | (v1, v2, ...)         | A[n]     |
            | map    | (k1, v1, k2, v2, ...) | M[key]   |
            | struct | (v1, v2, ...)         | S.x      |

    2. BY
        1. `ORDER BY`:
            1. guarantee global ordering
            2. but all data is pushed into one reducer, the output is a single sorted data
            3. `SET hive.mapred.mode=strict` makes compulsory to use `LIMIT` with `ORDER BY` to reduce the burden from reducer
        2. `SORT BY`
            1. data is sent to N reducers. Before sending the data, they are sorted based on the sort column
            2. the final output from all the reducers will not be a global sorted one
            3. the data in each reducer can have overlapping ranges
        3. `DISTRIBUTE BY`
            1. ensure each of N reducers gets non-overlapping ranges
            2. the same values in a distribute by column go to the same reducers
            3. the each reducer output is not sorted, end up N or more unsorted files with non-overlapping
        4. `CLUSTER BY`: `DISTRIBUTE BY` + `SORT BY`
    3. Functions
        1. Query
            1. `SHOW FUNCTIONS`
            2. `DESC FUNCTION func_name`
            3. `DESC FUNCTION EXTENDED func_name`
        2. UDFs: user defined functions
            1. General
                1. `version()`
                2. `current_database()`
                3. `current_user()`
                4. `md5()`
            2. Conversion
                1. `cast(value as another_type)`
                2. `binary(string|binary)`: cast parameter into a binary
            3. Math
                1. `round()`
                2. `floor()`
                3. `ceil(), ceiling()`
                4. `rand(), rand(seed)`
                5. `exp()`
                6. `ln()`
                7. `log10()`
                8. `log2()`
                9. `log(base, A)`
                10. `pow(), power()`
                11. `sqrt()`
                12. `bin()`
                13. `hex()`
                14. `unhex()`
                15. `conv(n, from_base, to_base)`
                16. `abs()`
                17. `pmod(a, b)`: positive value of `a mod b`
                18. `sin(), asin()`
                19. `cos(), acos()`
                20. `tan(), atan()`
                21. `degree()`: convert a from radians to degrees
                22. `radians()`: convert a from degrees to radians
                23. `positive()`
                24. `negative()`
                25. `sign()`
                26. `e()`
                27. `pi()`
                28. `factorial()`: factorial of a, 0 <= a <= 20
                29. `cbrt()`: cube root
                30. `shiftleft()`
                31. `shiftright()`
                32. `shiftrightunsigned()`
                33. `greatest()`
                34. `least()`
                35. `width_bucket()`
            4. string
                1. `length()`
                2. `ascii(str)`: return the numeric value of first character of str
                3. `chr(A)`
                4. `lower(), lcase(), upper(), ucase()`
                5. `concat(a, b, ...)`
                6. `instr()`
                7. `locate()`
                8. `replace()`
                9. `initcap()`
                10. `lpad(), rpad()`
                11. `ltrim(), rtrim(), trim()`
                12. `repeat()`
                13. `space(n)`
                14. `split(str, regex)`
                15. `concat_ws(sep, [str | array(string)]+))`
                16. `elt(n, s1, s2, ...)`
                17. `field(s, s1, s2, ...)`
                18. `format_number()`
                19. `quote()`
                20. `reverse()`
                21. `substr(), substring()`
                22. `substring(str, sep, n)`
                23. `translate()`
                24. `regexp_extract()`
                25. `regexp_replace()`
                26. `parse_url()`
            5. Timestamp
                1. `current_timestamp`
                2. `from_unixtime(unixtime[, 'yyyy-MM-dd hh:mm:ss'])`
                3. `unix_timestamp([timestamp, 'yyyy-MM-dd hh:mm:ss])`
                4. `date_format()`
                5. `current_date`
                6. `to_date(timestamp)`
                7. `date_add(start_date, days)`
                8. `date_sub(start_date, days)`
                9. `datediff(end_date, start_date)`
                10. `last_day()`: last day of month
            6. Conditional
                1. `if()`
                2. `isnull()`
                3. `isnotnull()`
                4. `nvl(value, default_value)`: return default value if value is null else return value
                5. `nullif(a, b)`: returns null if a=b, otherwise return a
                6. `coalesce(v1, v2, ...)`
                7. `CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END`
                8. `CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END`
                9. `assert_true(boolean condition)`: throw an exception if condition not met, otherwise return null
            7. Collection
                1. `size(Map<K, V>)`
                2. `size(Array<T>)`
                3. `map_keys(Map<K,V>)`: unorder array keys in map
                4. `map_values(Map<K,V>)`: unorder array values in map
                5. `array_contains(Array<T>, value)`
                6. `sort_array(Array<T>)`
            8. Others
                1. `get_json_object(json_txt, path)`
                2. `xpath(xml_str, path)`
        3. UDAFs: built-in aggregate functions
            1. `count([DISTINCT ...])`
            2. `sum()`
            3. `avg()`
            4. `max()`
            5. `min()`
            6. `collect_set()`
            7. `collect_list()`
        4. Over functions: `f() OVER (PARTITION BY f1, [f2, ...] [ORDER BY f1[, f2, ...]])`
            1. `rank()`
            2. `row_number()`
        5. UDTFs: built-in table-generating functions
            1. `explode`
                1. `explode(array|map)`
                2. `lateral view explode(split(col, delim)) n as nn`: `delim` in `split` support distinct characters
                3. `lateral view out explode(split(col, delim)) n as nn`: keep `NULL` value rows
            2. `posexplode`: similar to `explode`, but keep map join, not cartesian join
            3. `json_tuple(json_txt, k1, k2, ...)`
            4. `parse_url_tuple()`

                ```sql
                select
                    class,
                    student_name,
                    student_score
                from
                    default.classinfo
                    lateral view posexplode(split(student,',')) sn as student_index_sn,student_name
                    lateral view posexplode(split(score,',')) sc as student_index_sc,student_score
                where
                    student_index_sn = student_index_sc;
                ```
