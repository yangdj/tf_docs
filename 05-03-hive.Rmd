# Hive

1. Installation
    1. configuration
        1. `hive.metastore.warehouse.dir`:  where to store internal table data
2. Data Types: see [hive data type](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types)
    1. Primitive
        1. Integer 
            1. tinyint: 1-byte signed, -2^7 ~ 2^7 - 1, `100Y`
            2. smallint: 2-byte signed, -2^15 ~ 2^15 - 1, `100S`
            3. int / integer: 4-byte signed, -2^31 ~ 2^21 - 1, default for integer literal
            4. bigint: 8-byte signed, -2^63 ~ 2^63 - 1, `100L`
                1. integer larger than bigint must be handled with `decimal(38, 0)` with postfix BD, `100BD`
        2. Floating
            1. float: 4-byte single precision
            2. double: 8-byte double precision, default for floating literal
            3. double precision: alias for double, only available from Hive 2.2.0
            4. decimal(precision, scale):
                1. default `decimal(10, 0)`
                2. precision <=38, and should >= scale
                3. precision is the total width except the point; scale is width after the point
                4. if the integer part width `(precision - scale)` less than actual integer part width, `NULL` will be return
            5. numeric: same as decimal from Hive 3.0
        3. String
            1. char: `char(10)`, fix length, no more than 255, spaces may be padded
            2. varchar: `varchar(10)`, variable length, no more than 65535, no more than given length
            3. string: recommended, could be 2GB size
        4. Date & Time
            1. timestamp: stored as an offset from the unix epoch
                1. integer input interpreted as unix timestamp in **mileseconds when testing**, not seconds
                2. floating input interpreted as unix timestamp in seconds
                3. string input follow JDBC compliant `java.sql.Timestamp format` format `yyyy-mm-dd hh:mm:ss[.f{0,9}]` with optional nanoseconds
            2. date: a particular year/month/day value in the `yyyy-mm-dd` format, without a time of day component, range `0000-01-01 ~ 9999-12-31`
            3. interval: time units, `second(s)/minute(s)/hour(s)/day(s)/month(s)/year(s)`, `interval '1' day`, `interval '1-2' year to month` 
        5. boolean: `[true, false]`
        6. binary: array of bytes
    2. NULL: for missing value, `LazySimpleSerDe` interprets the string `\N`as `NULL` when importing
    3. Complex: collection type
        1. array: collection of similar data type
            1. `column[index]`: index starts from 0
            2. `score array<string>`:  specify column data type
            3. `collection items terminated by '$'`: specify elements separator when creating table
        2. struct: collection of named fields where each field can be of any primitive type
            1. `column.fieldname`
            2. `gamescore struct<game_name:string, score:int>`: specify column type and fields name and type
            3. `collection items terminated by '$'`: specify element separator
        3. map: collection of key value pairs
            1. `column[keys]`
            2. `score map<string, int>`
            3. `collection items terminated by '$'`: specify element separator
            4. `map keys terminated by ':'`: specify key separator
3. Object Operations
    1. database
        1. `CREATE DATABASE db_name [LOCATION 'oss://bilog/hivedb/db_name.db']`
    2. table
        1. `CREATE TABLE`
            1. `SHOW CREATE TABLE [db_name.]table_name`: show table creation statement
            2. internal table
                1. store data in `hive.metastore.warehourse.dir(/user/hive/warehouse/databasename.db)`
                2. on `DROP`, Hive deletes both the metadata and actual data 
                3. `TRUNCATE` works, but partitions remain as it is
            3. external table
                1. store data outside of Hive
                2. on `DROP`, Hive only deletes metadata, but the actual data stays as it is
                3. `TRUNCATE` does not work
                4. `SHOW CREATE TABLE` tells whether the table is internal or external. If find keyword `external` then external, else internal
                5. when creating external table without `LOCATION ...`, the table will be created like internal table
            4. ROW FORMAT
                1. `DELIMITED`: use **native SerDe**, `textfile` format, and specify delimiter, escape character, null character and so on
                2. `SERDE`: specify class to split a row into columns and serialize columns to a row, may need first load JAR file using `ADD JAR ***.jar`
                    1. class
                        1. `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`: default SerDe
                            1. SERPROPERTIES
                                1. `'field.delim'='\b'`: fields separator, use unicode `\u0008`
                                2. `'colelction.delim'='$'`: colletion separator
                                3. `'mapkey.delim'=':'`: map key separator
                                4. `'line.delim'='\n'`: line separator
                                5. `'serialization.null.format'='\\N'`: NULL identifier, `\N` default for `LazySimpleSerDe`
                                6. `'serialization.format'='\b'`: fields separator for serialization
            5. STORE AS: file format for the table
                1. system format

                    <div>
                    : (\#tab:hive-format) Hive System format

                    | Format       | SerDe                                                            | InputFormat                                                     | OutputFormat                                                     |
                    | ------------ | ---------------------------------------------------------------- | --------------------------------------------------------------- | ---------------------------------------------------------------- |
                    | textfile     | `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`             | `org.apache.hadoop.mapred.TextInputFormat`                      | `org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat`     |
                    | sequencefile | `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`             | `org.apache.hadoop.mapred.SequenceFileInputFormat`              | `org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat`      |
                    | rcfile       | `org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe` | `org.apache.hadoop.hive.ql.io.RCFileInputFormat`                | `org.apache.hadoop.hive.ql.io.RCFileOutputFormat`                |
                    | orc          | `org.apache.hadoop.hive.ql.io.orc.OrcSerde`                      | `org.apache.hadoop.hive.ql.io.orc.OrcInputFormat`               | `org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat`               |
                    | parquet      | `org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe`    | `org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat` | `org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat` |
                    | avro         | `org.apache.hadoop.hive.serde2.avro.AvroSerDe`                   | `org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat`    | `org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat`    |
                    
                    </div>

                2. custom format specifies `INPUTFORMAT` and `OUTPUTFORMAT` class, and choose `ROW FORMAT SERDE`

            ```sql
            CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
                (col_name data_type [COMMENT 'col_comment'], ...)
                [PARTITIONED BY (col_name data_type [COMMENT 'col_comment..'], ...)]
                [COMMENT 'table_comment..']
                [ROW FORMAT 
                    (SERDE 'serde_class' [with SERDEPROPERTIES('key1'='value1', ...)])
                    | (DELIMETED
                        [FIELDS TERMINATED BY field_separator_char]
                        [ESCAPED BY escape_char]
                        [COLLECTION ITEMS TERMINATED BY collection_separator_char]
                        [MAP KEYS TERMINATED BY map_key_separator_char]
                        [LINES TERMINATED row_separator_char]
                        [NULL DEFINED AS null_char]
                    )
                ]
                [STORED AS (file_format | INPUTFORMAT 'inputformat_class' OUTPUTFORMAT 'outputformat_class')]
                [LOCATION 'file_path']
                [TBLPROPERTIES ('key1'='value1', 'key2'='value2', ...)]
            ```

        2. `INSERT`
            1. `INSERT INTO foo VALUES (...), (...), ...`
            2. `INSERT INTO foo SELECT ...`
            3. `insert into foo PARTITION(dt) ...`
            4. `INSERT OVERWRITE TABLE foo SELECT ...` 
        3. `ALTER TABLE`
            1. `ALTER TABLE table_name RENAME TO new_table_name`: rename table
            2. `ALTER TABLE table_name SET TBLPROPERTIES('key'='value', ...)`: alter table properties, add or modify, not able to remove
            3. `ALTER TABLE table_name SET FILEFORMAT file_type`: alter storage properties, see the previous system format
            4. SerDe
                1. `ALTER TABLE table_name [partition(...)] SET SERDE 'serde_class_name' WITH SERDEPROPERTIES ('prob1'="value1", ...)`: set serde class and properties
                2. `ALTER TABLE table_name [partition(...)] SET SERDE 'serde_class_name'`: set SerDe class
                3. `ALTER TABLE table_name [partition(...)] SET SERDEPROPERTIES ('prob1'="value1", ...)`: set SerDe properties

    3. column
        1. `DESC table_name`: show columns info, names ,data_type and comment
        2. `ALTER TABLE foo ADD COLUMNS (col_name col_datatype, ...) [CASCADE|RESTRICT]`: add columns at the end of existing columns, if the table is partitioned, the columns get added at the end but before the partitioned column 
        3. `ALTER TABLE foo CHANGE [COLUMN] col_name new_column_name col_datatype [COMMENT col_comment] [FIRST|AFTER col] [CASCADE|RESTRICT]`
        4. `CASCADE` and `RESTRICT` are for alter columns in partitioned table
            1. `CASCADE`: changes are propagated to all the existing and future partitions
            2. `RESTRICT`: changes are visible for new partitions created, existing partitions are not impacted

    4. partition
        1. `ADD`: `alter table foo add if not exists partition(year='2021', month='06', day='08') [location ...]`
        2. `INSERT`
            1. all dynamic: `insert overwrite table foo partition(year, month, day) select ..., t.year, t.month, t.day from ...`
            2. partial dynamic: `insert overwrite table foo partition(year='2021', month='06', day) ...`
        3. `ALTER`
            1. alter column in all partition : `alter table foo partition (ds, hr) change column dec_column_name dec_column_name decimal(38,18)`
            2. alter column in specified partition column: `alter table foo partition (ds='2021-06-08', hr=12) change column dec_column_name dec_column_name decimal(38,18)`
4. Settings
    1. `set hive.mapred.mode=[nonstrict, strict]`:  when `strict`, SQL must add `limit` when using `order by`
    2. `set hive.strict.checks.cartesian.product=false`: enable cartesian join
    3. `set hive.execution.engine={mr,spark,tez}`: set execution engine
    4. `set hive.tez.container.size=6144`: set tez memory size
    5. `set mapred.reduce.tasks=60`: set numbers of reducers
    6. `set mapred.job.queue.name=queue0`
    7. `set tez.queue.name=queue0`
    8. `set hive.auto.convert.join=true`: join in mapper by load small table into memory when big table joins small table, this reduces shuffle operations
    9. `set hive.groupby.skewindata=true`: enable load balance when data skews, default false
    10. partition
        1. `set hive.exec.dynamic.partition=true`: enable dynamic partition
        2. `set hive.exec.dynamic.partition.mode=nostrick`: support all partitions dynamically, otherwise at least one should be static
        3. `set hive.exec.max.dynamic.partitions=100000`: max number for dynamic partitions in one SQL, default 1000
        4. `set hive.exec.max.dynamic.partitions.pernode=100000`: max number partitions support by one mapper or reducer, default 100
        5. `set hive.exec.max.created.files=100000`: max created file number globally ,default 100000
5. Functions
    1. explode
    2. collect_list
    3. collect_set
