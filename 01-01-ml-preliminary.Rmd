# (PART) Machine Learning {-}

# Preliminary

This chapter consists of basic knowledge and steps for machine learning.

## Scales

1. Nominal: different categories, {"red", "yellow", "blue"}
2. Ordinal: categories with natural ordering or ranking, {"small", "median", "big"}
3. Continuous

## Exploration

1. Variables measurement scales, continuous or categorical? If categorical, nominal, or ordinal?
2. Shape of the scales
      1. Frequency tables for categoricals
      2. Histogram for continuous
      3. Dotplots (density plots) for categoricals and continuous
3. Association with other variables
      1. Cross tabulations for categorical variables
      2. Scatter plots for continuous variables, suggesting relations, linear or non-linear?
      3. Boxplots and Dotplots (density plot) for continuous grouped by categorial variable, suggesting whether group mean increse or decrease with an ordinal variable?

## Modeling

1. Model specification - specifying models in two parts: an equation linking the response and explanatory variables and the probability distribution of the response variable.
2. Estimating parameters of the models.
3. Checking the adequacy of the model--how well the model ﬁts the actual data.
4. Making inferences; e.g., calculating conﬁdence intervals and testing hypotheses about the parameters.

## Overfitting

To overcome overfitting, try following methods:

1. Train with more data
2. Regularization (shrinkage), loss + penalty:
    1. Lasso (L1): \(\frac {1}{2}\sum\limits_{i=1}^{n}(y(x_n, \mathbf{w}) - y_n)^2+ \lambda ||w||_1 \quad \lambda \ge 0, ||w||_1=|w_1| + \cdots + |w_n|\)
    2. Ridge (L2): \(\frac {1}{2}\sum\limits_{i=1}^{n}(y(x_n, \mathbf{w}) - y_n)^2+ \frac {\lambda}{2} ||w||_2^2 \quad \lambda \ge 0, ||w||_2^2=w_1^2+\cdots + w_n^2\)

## Bootstrapping

Boostrapping generates multiple samples by resampling from one sample to estimate standard error of an estimator \(\hat \theta\). Let \(\hat {\theta_1^*}, ..., \hat {\theta_B^*}\) denote estimates, \(B\) is resamping times, then the bootstrap variance for \(\hat{\theta}\) is:

$$
\begin{aligned}
\mathrm{Var}(\hat \theta) &= \frac {1}{B - 1}\sum_{b = 1}^{B}{(\hat {\theta_b^*} - \bar {\theta^*})}^2 \\
\bar {\theta^*} &= \frac {1}{B}\sum_{b = 1}^{B}\hat {\theta_b^*}
\end{aligned}
$$

References:

1. [Machine Learning from Scratch](https://dafriedman97.github.io/mlbook/content/introduction.html)
2. [ljpzzz/machinelearning blog github](https://www.cnblogs.com/pinard)
3. [The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)
4. [A Complete Tutorial on Time Series Modeling in R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)
5. [Kernel functions in common use](https://en.wikipedia.org/wiki/Kernel_(statistics)#Kernel_functions_in_common_use)
6. [Density Estimation: Histogram and Kernel Density Estimator](http://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf)
7. [ISLR EXERCISES](http://yahwes.github.io/ISLR/) and [SOLUTIONS](https://blog.princehonest.com/stat-learning/)
