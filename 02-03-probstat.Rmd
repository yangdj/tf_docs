# Prob & Stats

## Distributions

This part consists of common distributions and their specical properties.

### Common Distributions {-#dist}

: (\#tab:dist) Common Distributions

| Name                   | Type      | Description             | Repr                     | Form                                                                                                                                                                                                                               | Expectation           | Variance                                     | CDF                                                                                      | [MLE](#mle)                                                   |
| ---------------------- | --------- | ----------------------- | ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------- | -------------------------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| Geometric              | Discrete  | First Occur Times       | \(G(X)\)                 | \(p(x=k) = (1-p)^{k-1}p\)                                                                                                                                                                                                          | \(\frac{1}{p}\)       | \(\frac{1-p}{p^2}\)                          |                                                                                          |                                                               |
| Binomial               | Discrete  | Replacement Sampling    | \(B(n, p)\)              | \(p(x=k)=\binom{n}{k}p^{k}(1-p)^{n-k}\)                                                                                                                                                                                            | \(np\)                | \(np(1-p)\)                                  |                                                                                          | \(\hat{p} = \frac{x}{n}\)                                     |
| Hypergeometric         | Discrete  | No Replacement Sampling | \(H(n, p)\)              | \(p(x=k)=\frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{N}}\)                                                                                                                                                                       | \(n\frac{M}{N}\)      | \(\frac{N-n}{N-1}n\frac{M}{N}\frac{N-M}{N}\) |                                                                                          |                                                               |
| Poisson                | Discrete  | Occur Times             | \(P(\lambda)\)           | \(p(x=k)=\frac{\lambda^{k}e^{-\lambda}}{k!}\)                                                                                                                                                                                      | \(\lambda \)          | \(\lambda\)                                  |                                                                                          | \(\lambda = \bar{x}\)                                         |
| Uniform                | Continual | ---                     | \(U(a, b)\)              | \(f(x) = \begin{cases}\frac{1}{b-a} &\quad (a\le x\le b) \\ 0 &\quad (\text{otherwise})\end{cases}\)                                                                                                                               | \(\frac{a+b}{2}\)     | \(\frac{(b-a)^2}{12}\)                       |                                                                                          |                                                               |
| Exponential            | Continual | ---                     | \(E(\lambda)\)           | \(f(x) = \begin{cases}\lambda e^{-\lambda x} &\quad (x > 0) \\ 0 &\quad (\text{otherwise})\end{cases} \\ 2\lambda X_i \sim E(1/2) \sim \chi_{2}^{2} \\ 2\lambda(X_1+X_2+\cdots+X_n) \sim \chi_{2n}^{2} \quad X_i \sim E(\lambda)\) | \(\frac{1}{\lambda}\) | \(\frac{1}{\lambda^{2}}\)                    |                                                                                          |                                                               |
| Normal                 | Continual | ---                     | \(N(\mu, \sigma^{2})\)   | \(f(x) = \frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}\)                                                                                                                                                 | \(\mu\)               | \(\sigma^{2}\)                               |                                                                                          | \(u=\bar{x} \\ \sigma^{2} = \frac{(x_{i} - \bar{x})^{2}}{n}\) |
| Chi Square             | Continual | ---                     | \(\chi_{n}^{2}\)         | \(\chi_{n}^{2} = \sum\limits_{i=1}^{n}X_{i}^{2} \\ X_i \sim N(0,1)\)                                                                                                                                                               | \(n\)                 | \(2n\)                                       |                                                                                          |                                                               |
| Non-Central Chi Square | Continual | ---                     | \(\chi_{n,\lambda}^{2}\) | \(\chi_{n, \lambda}^{2} = \sum\limits_{i=1}^{n}X_{i}^{2} \\ X_i \sim N(\mu_{i},1) \\ \lambda=\sum\mu_{i}^2\)                                                                                                                       | \(n + \lambda\)       | \(2n + 4\lambda\)                            |                                                                                          |                                                               |
| T Distribution         | Continual | ---                     | \(t_{n}\)                | \(t_{n}=\frac{X}{\sqrt{\chi_{n}^2/n}} \quad X\sim N(0,1)\)                                                                                                                                                                         |                       |                                              |                                                                                          |                                                               |
| Non-Central T          | Continual | ---                     | \(t_{n, \delta}\)        | \(t_{n, \delta}=\frac{Z + \delta}{\sqrt{\chi_{n}^2/n}} \\ P(t_{n, \delta_1} \le X) \ge P(t_{n, \delta_2} \le X) \quad \delta_1 \le \delta_2 \)                                                                                     |                       |                                              |                                                                                          |                                                               |
| F Distribution         | Continual | ---                     | \(F_{n,m}\)              | \(F_{n, m} = \frac{{\chi_{n}^{2}\;/n}}{\chi_{m}^{2}\;/m} \\ F_{1, n} = t_{n}^{2}\)                                                                                                                                                 |                       |                                              |                                                                                          |                                                               |
| Non-Center F           | Continual | ---                     | \(F_{n,m, \lambda}\)     | \(F_{n, m, \lambda} = \frac{{\chi_{n, \lambda}^{2}\;/n}}{\chi_{m}^{2}\;/m}\)                                                                                                                                                       |                       |                                              |                                                                                          |                                                               |
| Cauchy                 | Continual | ---                     | \(C(x_0,\lambda)\)       | \(f(x; x_0, \lambda) = \frac {1} {\pi\lambda \left[1+\left(\frac {x-x_0}{\lambda}\;\right)^2\right]} = \frac{1}{\pi}\left[\frac {\lambda} {(x-x_0)^2 + \lambda^2}\right] \\ f(x; 0, 1) = \frac {1} {\pi(1+x^2)}\)                  | \(\infty\)            | \(\infty\)                                   | \(F(x; x_0, \lambda) = \frac {1}{\pi}\arctan (\frac {x - x_0}{\lambda}) + \frac {1}{2}\) |                                                               |
| Weibull                | Continual | ---                     | \(W(\lambda, \theta)\)   | \(f(x; \lambda, \theta) = \begin{cases}\frac {\lambda y^{\lambda - 1}}{\theta^\lambda}\exp(-(\frac {y} {\theta})^\lambda) &\quad (x > 0) \\ 0 &\quad (\text{otherwise})\end{cases} \\ W(1, \theta) \sim E(1/\theta) \)             |                       |                                              |                                                                                          |                                                               |

### Exponential Distribution {-}

If the distribution satisfies:

$$
\lim_{h \to 0}P(x \le X \le x + h | X \gt x) / h = \lambda \\
$$

Then, the distribution can be inferred as follows:

$$
\begin{aligned}
&F'(x) / (1 - F(x)) = \lambda \Rightarrow F(x) = 1 - Ce^{-\lambda x} \\
&F(0) = 0 \Rightarrow C = 1 \Rightarrow F(x) = 1 - e^{-\lambda x} \\
&E(\lambda) = F'(x) = \lambda e^{-\lambda x}
\end{aligned}
$$

### Weibull Distribution {-}

In Weibull distribution, \(\lambda\) controls function shapes, and \(\theta\) controls scales. Similar to exponential distribution, it can be inferred from the condition:

$$
\begin{aligned}
&F'(x) / (1 - F(x)) = \lambda x^m  \quad (m > 0)\\
&F(x) = 1 - Ce^{-\frac {\lambda}{m + 1} x^{m + 1}} \quad (C = 1)
\end{aligned}
$$

## MLE

MLE (Maximum Likelihood Estimation) is to estimate \(\hat\theta\) making the following equation value maximum:

$$
L(\theta, y) = \prod_{i=1}^{n}f(y_i, \theta)
$$

Usually, log-likelihood estimation is used:

$$
l(\theta, y) = \log L(\theta, y) = \sum_{i=1}^{n}\log f(yi, \theta)
$$

To make \(l(\theta, y)\) maximum, just finding \(\hat \theta\), which satifies:

$$
l(\hat \theta) \ge l(\theta, y)  \quad \forall \theta  \in \Omega
$$

\(\hat \theta\) can be generally solved using:

$$
\begin{aligned}
&\frac {\partial{l(\theta, y)}}{\partial{\theta_j}} = 0 \quad j = 1,...,p \\
&\frac {\partial^2 l(\theta, y)} {\partial \theta_j \partial \theta_k}\bigg \lvert_{\theta = \hat \theta} \text{ is negative definite}
\end{aligned}
$$

\(U(\theta)\) is called Fisher's *Score Function*, and \(H(\theta)\) is the derivative of \(U(\theta)\).

$$
\begin{aligned}
&U(\theta) = \frac{\partial l(\theta, y)} {\partial \theta} \quad E(U(\theta)) = 0 \\
&H(\theta) = \frac{\partial U(\theta, y)} {\partial \theta} = \frac{\partial^{2} l(\theta, y)} {\partial \theta \partial \theta'}\\
\end{aligned}
$$

\(I(\theta)\) is called *Information Matrix*, and defined as:

$$
\begin{aligned}
I(\theta) &= -E(H) \\ 
&= \mathrm{Var}(U(\theta)) \\
&= E(U(\theta)U^{T}(\theta))
\end{aligned}
$$

```{theorem, label = "rc", name="Rao-Cramer Lower Bound"}
The variance of any unbiased estimator of a parameter \(\theta\) must be at least as large as :

$$
\mathrm{Var}(\theta) >= I^{-1}(\theta)
$$
```

The variance of mle estimation of \(\hat\theta\) is the inverse of the Information matrix, so it is efficient.

$$
\mathrm{Var}(\theta) = I(\theta)^{-1}
$$

MLE Properties

1. Invariance

      if \(\hat \theta\) is maximum likelihood value, and \(g(\theta)\) is any function of the parameters Î¸,then the maximum likelihood estimator of \(g(\theta)\) is \(g(\hat \theta)\).

2. Consistent: \(\lim\limits_{n \to \infty}\hat \theta = \theta\)
3. Asymptotically normal: \(\lim\limits_{n \to \infty}\hat \theta \sim N(\theta, I^{-1}(\theta))\)
4. Efficient: Variance-Covariance is the Rao-Cramer lower bound

Inference

1. \(Z\)-Statistic Test

    Special case of Wald test:  \(H_0: \theta_i = \theta^*\)

    $$
    z = \frac {\hat \theta_i - \theta^*} {\sqrt{\mathrm{Var}(\hat \theta)_i}} \stackrel{a} \sim N(0, 1)
    $$

2. Wald Test: \(H_0: \theta = \theta_0\)

   $$
   W = (\hat \theta - \theta_0)^T \mathrm{Var}^{-1}(\hat \theta)(\hat \theta - \theta_0) \stackrel {a} \sim \chi_p^2 \quad p \text{ is freedom of } \theta
   $$

3. Score Test

    $$
    U(\theta) \stackrel {a} \sim N_p(0, I(\theta))
    $$

    Under \(H_0 : \theta = \theta_0\), the quadratic form has approximately \(\chi_p^2\) distribution

    $$
    Q = u(\theta_0)^T I^{-1}(\theta_0)u(\theta_0)
    $$

4. Likelihood Ratio Test

    $$
    \lambda = \frac {L(\hat \theta_{\omega1}, y)} {L(\hat \theta_{\omega2}, y)} \quad \omega1 \subset \omega 2
    $$

    then, \(0 \le \lambda \le 1\)
    $$ -2\log \lambda \stackrel {a} \sim \chi_v^2 \quad v = dim(\omega 2) - dim(\omega 1)
    $$

    LIkelihood ratio test may be better than Wald and Score methods in small samples.

5. LM(Lagrange Multiplier) Test

## Bayes

Bayes basic rule is:

$$
P(A|B) = \frac {P(B|A)P(A)} {P(B)}
$$

## EFD

EFD(Exponential Family Distribution) has the following form, and \(a, b, s, t\) are known functions: 

$$
f(y, \theta) = s(y)t(\theta)e^{a(y)b(\theta)}
$$

or equivalent:

$$
\begin{aligned}
f(y, \theta) &= e^{a(y)b(\theta) + c(\theta) + d(y)} \\
s(y) &= e^{d(y)} \\
t(\theta) &= e^{c(\theta)}
\end{aligned}
$$

If \(a(y) = y\), the distribution is said to be in *canonical (standard)* form, and \(b(\theta)\) is sometimes called *natural parameter* of the distribution.

If there are other parameters, in addition to \(\theta\), they are regarded as nuisance parameters forming parts of the functions \(a, b, c, d\), and treated as known.

Properties

$$
\begin{aligned}
E(a(y)) &= -\frac {c^{'}(\theta)} {b^{'}(\theta)} \\
D(a(y)) &= \frac {b^{''}(\theta)c^{'}(\theta) - c^{''}(\theta)b^{'}(\theta)} {(b^{'}(\theta))^3}
\end{aligned}
$$

The following distributions are all in the canonical form.

: (\#tab:canonical) Common Canonical Distributions

| Distribution (\@ref(tab:dist)) | \(b(\theta)\)                      | \(c(\theta)\)                                                 | \(d(y)\)                    |
| ------------------------------ | ---------------------------------- | ------------------------------------------------------------- | --------------------------- |
| Binomial                       | \(\log {(\frac {\pi} {1 - \pi})}\) | \(nlog(1 - \pi)\)                                             | \(log \binom {n}{y}\)       |
| Poisson                        | \(\log \theta\)                    | \(-\theta\)                                                   | -\(\log y!\)                |
| Normal                         | \(\frac {\mu} {\sigma^2}\)         | \(-\frac {\mu^2} {2\sigma^2} - \frac{1}{2}log(2\pi\sigma^2)\) | \(-\frac {y^2}{2\sigma^2}\) |
| Exponential                    | \(-\lambda\)                       | \(\log(\lambda)\)                                             | 0                           |
