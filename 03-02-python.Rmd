# Python

## Environment Management

### venv {-}

`venv` module can be used to create virtual environment.

1. create virtual enviromeng

    ```sh
    python3 -m venv virtual-name
    ```

2. activate virtual environment

    ```sh
    source virtual-name/bin/activate; .virtual-name\scripts\activate
    ```

3. quit the environment

    ```sh
    deactivate
    ```

### pip {-}

`pip` is used to manage packages.

Configuration:

* `~/.pip/pip.conf`: for Linux and OSX
* `~\pip\pip.ini`: for Windows

```ini
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host = https://pypi.tuna.tsinghua.edu.cn
```

Commands:

1. `pip install package`
2. `pip uninstall package`
3. `pip install --upgrade package`
4. `pip list package`
5. `pip show package`
6. `pip search package`
7. `pip freeze > requirements.txt`
8. `pip install -r requirements.txt`

### Conda {-}

Virtual environment:

1. `conda --version`
2. `conda update conda`
3. `conda create --name vtl-name python=3.5`
4. `conda activate vtl-name`
5. `conda activate, conda deactivate`: switch back to default environment (base)
6. `conda info --envs`: list all environments
7. `conda remove --name vtl-name --all`: remove environment
8. `conda config --append channels conda-forge` - adds new channel when "packages not available ..."

Package management:

1. `conda install pkg(=1.1.1)`
2. `conda remove pkg`
3. `conda update pkg`
4. `conda list pkg`
5. `conda search pkg`

## Jupyter Deploy

### Package Dependent {-}

1. ipython
2. line_profiler
3. numpy
4. matplotlib
5. pandas
6. jupyter
7. scipy
8. scikit-learn
9. seaborn
10. tensorflow
11. keras

### Environment Variable {-}

```sh
export IPYTHONDIR=~/.ipython - default value
export MPLCONFIGDIR=~/.config/matplotlib - default value
export JUPYTER_CONFIG_DIR=~/.jupyter - default value
```

### ipython {-}

Commands:

1. `ipython profile create <name>`: create configuration file under `$IPYTHONDIR `
2. `ipython profile locate`
3. `ipython profile list`

Configuration: 

```python
# ${IPYTHONDIR}/profile_default/ipython_config.py
c.InteractiveShellApp.extensions = ["line_profiler"]
c.InteractiveShellApp.exec_lines = [
import numpy as np
import pandas as pd
]
```

### matplotlib {-}

Font install

```sh
mkdir -p /usr/share/fonts/STSong 
mv STSONG.ttf /usr/share/fonts/STSong
mkfontscale 
mkfontdir 
fc-cache -fv

cp STSONG.ttf site-packages/matplotlib/mpl-data/fonts/ttf
rm -rf ${MPLCONFIGDIR}/{fontList.json,tex.cache}
rm -rf ~/.cache/matplotlib - if exists
```

Configuration

```
# ${MPLCONFIGDIR}/matplotlibrc
font.family: STSong
axes.unicode_minus: False
```

### jupyter {-}

Commands:

1. `juypter notebook --generate-config`: generate config file in directory `${JUPYTER_CONFIG_DIR}`
2. `jupyter notebook --no-browser --port=5000 --ip=0.0.0.0`: default port 8888
3. `jupyter notebook password`:  set jupyter login password, or use the following method
4. `python -c "from notebook.auth import passwd; print(passwd('jupyter'))"`

Configuration

```python
# ${JUPYTER_CONFIG_DIR}/jupyter_notebook_config.py
import os                                                                                 
import sys                                                                                
                                                                                          
#  for Mac, set browser value definitely; otherwise jupyter will not open browser automatically                                 
if sys.platform == "darwin":                                                              
    c.NotebookApp.browser = "Safari"
                                                    
c.NotebookApp.notebook_dir = "/server/proj/py-lrn"
#c.NotebookApp.password = "sha1:9e799b2236aa:01085662782c7813128637089192f836901b196d"
```

ipython-notebook.service

```ini
# /usr/lib/systemd/system/ipython-notebook.service
[Unit]
Description=Jupyter Notebook Server

[Service]
Type=simple
Environment="LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib"
Environment="PATH=/usr/local/bin:/usr/bin:$PATH"
ExecStart=/server/proj/.env/py-lrn/bin/jupyter notebook --ip=0.0.0.0 --no-browser                                                   
User=yang
Group=yang

[Install]
WantedBy=multi-user.target
```

## Database {#py-db}

1. MySQL
    * `pymysql`
    * `mysqlclient`
2. PostgreSQL
    * `psycopg2-binary`
3. SQLServer
    * `pymssql`
    * dependencies
        * `freetds-devel`: for `sqlfont.h`
        * `unixODBC-devel`: for sql.h
        * `C_INCLUDE_PATH`: search `sql.h` and other header files

        ```sh
        yum install freetds-devel unixODBC-devel
        export C_INCLUDE_PATH=/usr/include
        ```
4. Hive
    * `pyhive[hive]`
    * dependencies
        * `thrift-sasl`: when meets "TSaslClientTransport' object has no attribute 'readAll'"

        ```sh
        yum install cyrus-sasl-devel cyrus-sasl-plain thrift-sasl
        ```
5. SQLAlchemy
    * connection string: `prefix://username:password@host:port/databasename?parameter=value`
    * prefix:
        * PostgreSQL: `postgresql+psycopg2`
        * MySQL: `mysql`
        * SQLServer: `mssql+pymssql`
    * parameter:
        * MySQL: `charset=utf8`

## Airlfow

### Installation {-}

1. `SLUGIFY_USES_TEXT_UNIDECODE=yes pip install apache-airflow`: since airflow >= 1.10
2. `pip install Fernet cryptography`
3. `pip install "apache-airflow[password]"`: for authentication
4. `pip install "apache-airflow[hive]"`: for hive
5. `pip install psycopg2-binary`: for postgresql
6. `pip install "apache-airflow[mssql]"`: for mssql
7. `pip install "apache-airflow[celery]"`: `4.4.7` for `1.9`
8. `pip install "apache-airflow[redis]"`: when use redis for CeleryExecutor
9. `pip install "apache-airflow[rabbitmq]"`: when use rabbitmq for CeleryExecutor

Installing `apache-airflow[rabbitmq]` may encounter a source error, then:

1. modify `except Exception, e` to `except Exception as e`
2. `python setup.py build`
3. `python setup.py install`

### Setup {-}

1. set `AIRFLOW_HOME` path

    ```sh
    export AIRFLOW_HOME= /YOUR/AIRFLOW/PROJECT/HOME/
    ```

2. initialization: 
 
    ```sh
    airflow initdb # < 2.0
    airflow resetdb # rebuild the database< 2.0
    airflow db init # >= 2.0
    ```
    
    in generated file `airflow.cfg`, update following values according to demands:

    1. sql_alchemy_conn
        1. postgresql:  `sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/proj`
        2. mysql: `sql_alchemy_conn = mysql://airflow:airflow@localhost/airflow?charset=utf8`

    2. executor
        1. `executor = CeleryExecutor`
            1. use redis:
                1. without password: `broker_url = redis://127.0.0.1:6379/15`
                2. with password: `broker_url = redis://:password@127.0.0.1:6379/15`
            2. use rabbigmq: `broker_url = amqp://myairflow:airflowpass@localhost:5672/myvhost`

                ```sh
                # set rabbimq airflow user when using rabbitmq
                sudo rabbitmqctl add_user myairflow airflowpass
                sudo rabbitmqctl add_vhost myvhost
                sudo rabbitmqctl set_user_tags myairflow airflow
                sudo rabbitmqctl set_permissions -p myvhost myairflow ".*" ".*" ".*"
                ```

    3. result_backend (celery_result_backend < 1.10)
        1. postgresql: `result_backend = db+postgresql://airflow:airflow@localhost:5432/proj`
        2. mysql: `result_backend = db+mysql://airflow:airflow@localhost/airflow?charset=utf8`
        3. redis: `result_backend = redis://127.0.0.1:6379/15`

    4. authenticate under `[webserver]` part, not `[api]` part, otherwise it may be report all kinds of error, e.g., not found `client_auth`
        1. `<2.0`
            1. `authenticate = True`
            2. `auth_backend = airflow.contrib.auth.backends.password_auth`
        2. `>= 2.0`
            1. `auth_backend = airflow.api.auth.backend.basic_auth`
        
    5. timezone
        1. `default_timezone = Asia/Shanghai` (1.10)
        2. `default_ui_timezone = Asia/Shanghai` (>=2.0)

    6. `load_examples = False`: hide example dags, config before first start webserver, otherwise:
        1. manually delete example dag records in `dag` table
        2. `airflow resetdb`: rebuild database, but will lose other useful settings, avoid in production environment

    7. email under `[email]` part
        1. use `tls`

            ```ini
            smtp_starttls = True
            smtp_ssl = False
            smtp_port = 587
            ```
        2. use `ssl`

            ```ini
            smtp_starttls = False
            smtp_ssl = True
            smtp_port = 465
            ```
        
        3. `smtp_host`
            1. `smtp.exmail.qq.com`: for qq
        
        4. other settings

            ```ini
            smtp_user = hello@world.com
            smtp_password = password
            smtp_mail_from = hello@world.com
            ```

3. create airflow user account

    ```python
    # create_user.py
    import airflow
    from airflow import models, settings
    from airflow.contrib.auth.backends.password_auth import PasswordUser
    user = PasswordUser(models.User())
    user.username = 'airflow'
    user.email = 'new@example.com'
    # user.password = 'airflow'
    user._set_password = 'airflow'
    session = settings.Session()
    session.add(user)
    session.commit()
    session.close()
    ```

    ```sh
    python create_user.py
    ```
    or directly create user

    ```sh
    airflow users create \
        --username admin \
        --firstname Peter \
        --lastname Parker \
        --role Admin \
        --email spiderman@superhero.org
    ```

4. firewall
    1. `sudo firewall-cmd --permanent --add-port=8000/tcp`: airflow webserver, 8080 default 
    2. `sudo firewall-cmd --permanent --add-port=5555/tcp`: airflow flower, 5555 default
    3. `sudo systemctl reload firewalld`

### Deployment {-}

#### Start services directly {-}

1. start webserver with port `8080` default: `airflow webserver --port 8080`
2. start worker: `airflow worker`
3. start scheduler: `airflow scheduler` (need restart after `airflow resetdb`)
4. start flower, with port `5555` default: `airflow flower`
   
```sh
nohup airflow webserver >> /data/jst/airflow/server_logs/webserver.log 2>&1 &
nohup airflow worker >> /data/jst/airflow/server_logs/worker.log 2>&1 &
nohup airflow scheduler >> /data/jst/airflow/server_logs/scheduler.log 2>&1 &
nohup airflow flower >> /data/jst/airflow/server_logs/flower.log 2>&1 &
```

#### Systemd {-}

Using systemd to deploy airflow server, and copy the following service file to `/usr/lib/systemd/system/`:

```sh
cp airflow-*er.service /usr/lib/systemd/system/
```

1. `airflow.conf` file

    ```
    D /run/airflow 0755 airflow airflow
    ```

    ```sh
    cp airflow.conf /etc/tmpfiles.d/
    mkdir /run/airflow
    chown airflow: /run/airflow
    ```

2. `airflow` file

    ```sh
    AIRFLOW_HOME=/server/proj/airflow/test
    SCHEDULER_RUNS=10
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib
    PYTHONPATH=${AIRFLOW_HOME}:${AIRFLOW_HOME}/plugins:${PYTHONPATH}
    PATH=/server/proj/.env/airflow/bin:/usr/bin:/user/local/bin:/usr/local/sbin:/usr/sbin:${PATH}
    ```

    ```sh
    cp airflow /etc/sysconfig/
    ```

3. `airflow-webserver.service` file
   
    ```ini
    [Unit]
    Description=Airflow webserver daemon 
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow webserver -p 8000 --pid /run/airflow/webserver.pid
    Restart=on-failure
    RestartSec=5s
    PrivateTmp=true

    [Install]
    WantedBy=multi-user.target
    ```

4. `airflow-worker.service` file

    ```ini
    [Unit]
    Description=Airflow celery worker daemon
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow worker
    Restart=on-failure
    RestartSec=10s

    [Install]
    WantedBy=multi-user.target
    ```

5. `airflow-scheduler.service` file

    ```ini
    [Unit]
    Description=Airflow scheduler daemon
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow scheduler
    Restart=always
    RestartSec=5s

    [Install]
    WantedBy=multi-user.target
    ```

6. `airflow-flower.service` file 

    ```ini
    [Unit]
    Description=Airflow celery flower
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow flower
    Restart=on-failure
    RestartSec=5s

    [Install]
    WantedBy=multi-user.target
    ```

### Concepts {-}

#### DAG {-}

In airflow, a `DAG` (Directed Acyclic Graph) is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.

DAGs are defined in standard python file placed in Airflow's `DAG_FOLDER`. Airflow will execute the code in each file to dynamically build the `DAG` objects.

If a dict of `default_args` is passed to a DAG, it will apply them to any of its operators. This makes it easy to apply a common parameter to many operators without having to type it many times. It includes following values:

1. `"owner": "airflow"`: `str`, whom the tasks belong to, everything you can use
2. `"start_date": datetime(2020, 3, 15)`: `datetime`, the **execution date** start date, determine the first task instance
3. `"end_date": datetime(2020, 3, 20)`: `datetime`, if specified, the scheduler won't go beyond this date
4. `"depends_on_past": False"`: `bool`, when set to true, task instances will run sequentially while relying on the previous task’s schedule to succeed. The task instance for the start_date is allowed to run
5. `"wait_for_downstream: False"`: `bool`, when set to true, an instance of task X will wait for tasks immediately downstream of the previous instance of task X to finish
6. `"email": ["airflow@example.com"]`: to who the mail sends to
7. `"email_on_failure": True`: `bool`, send the mail when the task fails
8. `"email_on_retry: True`: `bool`, whether retries when the task fails
9. `"retries": 5`: `int`, how many times to retry
10. `"retry_delay": timedelta(minutes=5)`: `datetime.timedelta`, when to retry when the task fails
11. `"execution_timeout": timedelta(hours=3)`: `datetime.timedelta`, max time allowed for the execution of this task instance, if it goes beyond it will raise and fail
12. `"queue": "bash_queue"`: `str`, tasks belong to which queue
13. `"pool": "backfill"`: `str`, default `"default pool"`, which pool tasks belong to
14. `"priority_weight" =  10`: `int`, priority weight of this task against other task. This allows the executor to trigger higher priority tasks before others when things get backed up
15. `"trigger_rule"`: `str`, `all_success` default,  define the rule by which dependencies are applied for the task to get triggered
    * all_success
    * all_failed
    * all_done
    * one_success
    * one_failed
    * dummy
16. `"on_failure_callback"`: `callable`
17. `"on_success_callback"`: `callble`
18. `"on_retry_callback"`: `callble`
19. `"task_concurrency": 10`: `int`, when set, a task will be able to limit the concurrent runs across execution_dates

Other parameters for DAG:

* `dag_id = "tutorial"`: `str`, the id of the DAG
* `description`: `str`, the description for the DAG to be shown on the webserver 
* `schedule_interval` defines when to run the dag.
    1. `* * * * *`: similar to cron format
    2. `datetime.timedelta`
    3. preset cron
        1. `None`: don't schedule, use for exclusively "externally triggered" DAGs
        2. `@once`: schedule once and only once
        3. `@hourly`: run once an hour at the beginning of the hour, `0 * * * *`
        4. `@daily`: run once a day at midnight, `0 0 * * * `
        5. `@weekly`: run once a week at midnight on **Sunday** morning, `0 0 * * 0`
        6. `@monthly`: run once a month at midnight of the first day of the month, `0 0 1 * *`
        7. `@yearly`: run once a year at midnight of January 1, `0 0 1 1 *`
* `start_date`
* `end_date`
* `catchup = False`: `bool`, when `False`, the scheduler will only create a DAG Run for the most current instance of the DAG interval series, not all for any interval that has not been run or has been cleared
* `dagrun_timeout`: `datetime.timedelta`, specify how long a DagRun should be up before timeout / failing, so that new DagRuns can be created
* `parallelism`: glocal configuration `parallelism` in `airflow.cfg`, how many tasks to run for every airflow worker
* `concurrency = 10`: `int`, the number of task instances allowed to run concurrently per DAG, read `dag_concurrency` from `airflow.cfg` if not set
* `max_active_runs = 5`: `int`, maximum number of active DAG runs per DAG, read `max_active_runs_per_dag` from `airflow.cfg` if not set, beyond this number, the scheduler won’t create new active DAG runs. e.g., when using `backfill`, or manually trigger the dag, the running number dag will be controlled no more than this value. `max_active_runs = 1` will make sure the same DAG running sequencically, one after another, the later one will not start before the previous one successes

```python
# tutorial.py
from airflow import DAG
from datetime import datetime, timedelta

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datatime(2020, 3, 15),
    "email": ["airflow@example.com],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG("tutorial", default_args = default_args, schedule_interval = "10 * * * *")
```

Since `1.8`, DAGs can be used as context managers to automatically assign new operators to that DAG.

```python
with DAG("tutorial", default_args=default_args) as dag:
    op = DummyOperator("op")

op.dag is dag # True
```

Dag operations:

1. `python tutorial.py`: test whether the dag pipeline parses
2. `airflow list_dags`: print the list of **active** dags
3. `airflow list_tasks tutorial`: print the list of tasks in the dag_id tutorial
4. `airflow list_tasks tutorial --tree`: print the hierarchy of tasks in the dag_id tutorial

#### Task {-}

Tasks are generated when instantiating operator objects. A task must include or inherit the
arguments `task_id` and `owner`, otherwise Airflow will raise an exception.

*Task instance* is a task with the following characters:

1. has been assigned to a DAG
2. has a state associated with a specific run of the DAG

Tasks have following status:

* success
* running
* failed
* skipped
* retry
* queued
* no status

The dependencies between tasks can use:

1. `t2.set_upstream(t1)` <=> `dag.set_dependency("task_id_t1", "task_id_t2")` <=> `t2 << t1 (since 1.8)`
2. `t1.set_downstream(t2)` <=> `t1 >> t2 (since 1.8)`

Task test

`airflow test` command runs task instances locally, outputs their log to stdout (on screen), doesn’t bother with dependencies, and doesn’t communicate state (running, success, failed, …) to the database. It simply allows testing a single task instance.

The actual task instances run on a specific date. The date specified in this context is an `execution_date`, which simulates the scheduler running your task or dag at a specific date + time:

1. `airflow test tutorial print_date 2015-06-01`: test `print_date` on execution_date `2015-06-01`

Task run

Similar to task test, but runs as usual task

1. `airflow run tutorial print_date 2015-01-01`

Task backfill

The date range in this context is a start_date and optionally an end_date, which are used to populate the run schedule with task instances from this dag.

1. `airflow backfill tutorial -s 2015-06-01 -e 2015-06-07`

#### Operators {-}

Operators determines what actually gets done. It describes a single task in a workflow. Operators are only loaded by Airflow if they are assigned to a DAG.

Arguments common for every operators:

1. `task_id="task_name"`: a unique meaningful id for the task, first argument for every opeartor
2. `owner="airflow"`: the owner of the task, using the unix username is recommended, can be inherited from DAG's `default_args`
3. `dag=dag`: which dag the task belongs to

Airflow provides operators for many common tasks, including:

1. `DummyOperator`:
   
   ```python
   from airflow.operators.dummy_operator import DummyOperator
   ```

2. `BashOperator`: execute a bash command
    1. `bash_command`: can be shell statements or a `.sh` file, or Jinja template. The file location is relative to the directory containing the pipeline file (`tutorial.py`)
    2. `params={"my_param": "parameter I passed in"}`: allow to pass a dictionary of parameters or objects to templates

    ```python
    from airflow.operators.bash_operator import BashOperator

    t1 = BashOperator(
        task_id="print_hello",
        bash_command="date",
        dag=dag
        )

    templated_command = """
        {% for i in range(5) %}
            echo "{{ ds }}"
            echo "{{ macros.ds_add(ds, 7) }}"
            echo "{{ params.my_param }}"
        {% endfor %}
        """

    t2 = BashOperator(
        task_id='templated',
        bash_command=templated_command,
        params={'my_param': 'Parameter I passed in'},
        dag=dag)
    
    t2.set_upstream(t1)
    ```

3. `PythonOperator`: calls an arbitary python function

    ```python
    from airflow.operators.python_operator import PythonOperator
    ```

4. `EmailOperator`: send an email

    ```python
    from airflow.operators.email_operator import EmailOperator
    ```
    
5. `HTTPOperator`: send an HTTP request
6. `SubDagOperator`

    ```python
    from airflow.operators.subdag_operator import SubDagOperator
    ```

7. `SSHOperator`

    ```python
    from airflow.contrib.operators.ssh_operator import SSHOperator
    ```
   
8. SQL Operators:
    1. `SqliteOperator`
    2. `PostgresOperator`
    3. `MySqlOperator`
    4. `MsSqlOperator`
    5. `OracleOperator`
    6. `JdbcOperator`
    7. `HiveOperator`
9.  `Sensor`: wait for a certain time, file, database row, etc...
   
#### Default Variables {-}

The Airflow engine passes a few variables by default that are accessible in all templates:

1. `{{ ds }}`: the execution date as `YYYY-MM-DD`
2. `{{ ds_nodash }}`: the execution date as `YYYYMMDD`
3. `{{ execution_date }}`: the execution_date, (datetime.datetime)
4. `{{ dag}}`: the DAG object
5. `{{ task }}`: the Task object, `{{ task.owner}}`, `{{ task.task_id}}`, ...
6. `{{ task_instance }}, {{ ti }}`: the task_instance object, `{{ ti.hostname}}`
7. `{{ task_instance_key_str }}`: a unique, human-readable key to task instance formatted `{dag_id}_{task_id}_{ds}`
8. `{{ params }}`: a reference to the user_defined params dictionary
9. `{{ conf }}`: the full configuration object located at `airflow.configuration.conf` which represents the content of your `airflow.cfg`
10. `{{ var.value.my_var }}`: global defined variables represented as a dictionary
11. `{{ var.json.my_var.path }}`: global defined variables represented as a dictionary with deserialized JSON object, append the path to the key within the JSON object
12. `{{ run_id }}`: the `run_id` of the current DAG run
13. `{{ dag_run }}`: a reference to the DagRun object
14. `{{ test_mode }}`: whether the task instance was called using the CLI’s `test` subcommand

#### Macros {-}

1. `macros.datetime`: The standard lib’s `datetime.datetime`
2. `macros.timedelta`: The standard lib’s `datetime.timedelta`
3. `macros.dateutil`: A reference to the `dateutil` package
4. `macros.time`: The standard lib’s `time`
5. `macros.uuid`: The standard lib’s `uuid`
6. `macros.random`: The standard lib’s `random`

#### Hooks {-}

Hooks are interfaces to external platforms and databases like Hive, Postgres, HDFS. They also use `airflow.models.Connection` model to retrieve hostnames and authentication information. Hooks keep authentication code and information out of pipelines, centralized in the metadata database.

#### Pools {-}

Airflow pools can be used to limit the execution parallelism on arbitrary sets of tasks. The list of pools is managed in the UI (`Menu -> Admin -> Pools`) by giving the pools a name and assigning it a number of worker slots. The `pool` parameter can be used in conjunction with `priority_weight` to define priorities in the queue, and which tasks get executed first as slots open up in the pool. The default `priority_weight` is 1, and can be bumped to any number. When sorting the queue to evaluate which task should be executed next, we use the `priority_weight`, summed up with all of the `priority_weight` values from tasks downstream from this task. You can use this to bump a specific important task and the whole path to that task gets prioritized accordingly.

#### Queues {-}

When using the CeleryExecutor, the celery queues that tasks are sent to can be specified. `queue` is an attribute of BaseOperator, so any task can be assigned to any queue. The default queue for the environment is defined in the `airflow.cfg`'s `celery -> default_queue`. This defines the queue that tasks get assigned to when not specified, as well as which queue Airflow workers listen to when started.

Workers can listen to one or multiple queues of tasks. When a worker is started (using the command `airflow worker`), a set of comma delimited queue names can be specified (e.g. `airflow worker -q spark`). This worker will then only pick up tasks wired to the specified queue(s).

#### Connections {-}

The connection information to external systems is stored in the Airflow metadata database and managed in the UI (`Menu -> Admin -> Connections`) A `conn_id` is defined there and hostname / login / password / schema information attached to it. Airflow pipelines can simply refer to the centrally managed `conn_id` without having to hard code any of this information anywhere.

Many connections with the same `conn_id` can be defined and when that is the case, and when the hooks uses the `get_connection` method from `BaseHook`, Airflow will choose one connection randomly, allowing for some basic load balancing and fault tolerance when used in conjunction with retries.

Airflow also has the ability to reference connections via environment variables from the operating system. The environment variable needs to be prefixed with `AIRFLOW_CONN_` to be considered a connection. When referencing the connection in the Airflow pipeline, the `conn_id` should be the name of the variable without the prefix. For example, if the `conn_id is` named `postgres_master` the environment variable should be named `AIRFLOW_CONN_POSTGRES_MASTER` (note that the environment variable must be all uppercase). Airflow assumes the value returned from the environment variable to be in a URI format (e.g. `postgres://user:password@localhost:5432/master` or `s3://accesskey:secretkey@S3`).

`airflow connections` operations:

1. `-l, --list`
2. `-a, --add`
3. `-d, delete`
4. `--conn_id`: required to add or delete a connection, use lowercase characters and separated words with underscores `_`
5. `--conn_type`: required to add a connection without conn_uri
6. `--conn_host`
7. `--conn_port`
8. `--conn_schema`: specify database name
9. `--conn_login`: login user
10. `--conn_password`
11. `conn_extra`: Extra field as json dictionary, optional when adding a connection, e.g. `{"charset": "utf8"}` for MySQL
12. `--conn_uri`: required to add a connection without conn_type

```sh
# add a connection
airflow connections -a --conn_id prd --conn_type 'postgres' --conn_host '127.0.0.1' --conn_port 5432 --conn_schema postgres --conn_login postgres --conn_password 1234
```

```python
from airflow.hooks.base_hook import BaseHook

conn = BaseHook.get_connection("conn_id")
conn.conn_id
conn.conn_type
conn.host
conn.port
conn.schema
conn.login
conn.passowrd
conn.extra
conn.get_extra()
conn.extra_dejson.get('charset', False)
conn.get_url()
```

#### Xcoms {-}

#### Variables {-}

Variables are a generic way to store and retrieve arbitrary content or settings as a simple key value store within Airflow. Variables can be listed, created, updated and deleted from the UI (`Admin -> Variables`), code or CLI. While your pipeline code definition and most of your constants and variables should be defined in code and stored in source control, it can be useful to have some variables or configuration items accessible and modifiable through the UI.

```python
from airflow.models import Variable
foo = Variable.get("foo")
bar = Variable.get("bar", deserialize_json=True)
```

The second call assumes json content and will be deserialized into bar. Note that Variable is a sqlalchemy model and can be used as such.

`airflow variables` operations:

1. `-s, --set`
2. `-g, --get`
3. `-x, --delete`: delete a variable
4. `-i, --import`: import variables from JSON file
5. `-e, --export`: export variables to JSON file
6. `-d, --default`: default value returned if variable do not exist

## Base {#py-base}

Functions

1. `id()`
2.  `sys.getsizeof() - return the size of objects in bytes`
3. `type()`
4. `repr()`
5. `str()`
6. `dir()`
7. `issubclass()`
8. `isinstance()`: `isinstance(True, int) => True`
9. `getattr()`
10. `hasattr()`
11. `class.__mro__`
12. `class.mro()`
13. `class.__bases__`
14. `os.path.abspath(__file__)`
15. `os.path.dirname()`
16. `os.path.join()`
17. `".".join()`

False value

1. `0; 0.0; 0j`
2. `""`
3. `False`
4. `[]`
5. `()`
6. `{}`
7. `None`

## Packages {#py-pkgs}

### http.server {-}

```sh
python -m http.server 8888
```

### pytz {-}

```python
import pytz
import datetime

TZ = pytz.timezone("Asia/Shanghai")
NOW = datetime.datetime.now(TZ)
NOW_DATE = NOW.strftime("%Y-%m-%d")
```

### pillow {-}

see [pillow](https://pillow.readthedocs.io/en/latest/index.html)

```python
from PIL import Image
import imageio

im = Image.open("ll.gif")
try:
    i = 0
    while True:
        im.seek(i)
        im.save("{}.png".format(i))
        i = i + 1
except:
    pass

images = []

for j in range(i):
    images.append(imageio.imread("{}.png".format(j)))
 
imageio.mimwrite("ll-out.gif", images, "GIF", duration=0.3)
```
