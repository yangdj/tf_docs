# Python

## Environment Management

### venv {-}

`venv` module can be used to create virtual environment.

1. create virtual enviromeng

    ```sh
    python3 -m venv virtual-name
    ```

2. activate virtual environment

    ```sh
    source virtual-name/bin/activate; .virtual-name\scripts\activate
    ```

3. quit the environment

    ```sh
    deactivate
    ```

### pip {-}

`pip` is used to manage packages.

Configuration:

* `~/.pip/pip.conf`: for Linux and OSX
* `~\pip\pip.ini`: for Windows

```ini
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host = https://pypi.tuna.tsinghua.edu.cn
```

Commands:

1. `pip install package`
2. `pip uninstall package`
3. `pip install --upgrade package`
4. `pip list package`
5. `pip show package`
6. `pip search package`
7. `pip freeze > requirements.txt`
8. `pip install -r requirements.txt`

### Conda {-}

Virtual environment:

1. `conda --version`
2. `conda update conda`
3. `conda create --name vtl-name python=3.5`
4. `conda activate vtl-name`
5. `conda activate, conda deactivate`: switch back to default environment (base)
6. `conda info --envs`: list all environments
7. `conda remove --name vtl-name --all`: remove environment
8. `conda config --append channels conda-forge` - adds new channel when "packages not available ..."

Package management:

1. `conda install pkg(=1.1.1)`
2. `conda remove pkg`
3. `conda update pkg`
4. `conda list pkg`
5. `conda search pkg`

## Jupyter Deploy

### Package Dependent {-}

1. ipython
2. line_profiler
3. numpy
4. matplotlib
5. pandas
6. jupyter
7. scipy
8. scikit-learn
9. seaborn
10. tensorflow
11. keras

### Environment Variable {-}

```sh
export IPYTHONDIR=~/.ipython - default value
export MPLCONFIGDIR=~/.config/matplotlib - default value
export JUPYTER_CONFIG_DIR=~/.jupyter - default value
```

### ipython {-}

Commands:

1. `ipython profile create <name>`: create configuration file under `$IPYTHONDIR `
2. `ipython profile locate`
3. `ipython profile list`

Configuration: 

```python
# ${IPYTHONDIR}/profile_default/ipython_config.py
c.InteractiveShellApp.extensions = ["line_profiler"]
c.InteractiveShellApp.exec_lines = [
import numpy as np
import pandas as pd
]
```

### matplotlib {-}

Font install

```sh
mkdir -p /usr/share/fonts/STSong 
mv STSONG.ttf /usr/share/fonts/STSong
mkfontscale 
mkfontdir 
fc-cache -fv

cp STSONG.ttf site-packages/matplotlib/mpl-data/fonts/ttf
rm -rf ${MPLCONFIGDIR}/{fontList.json,tex.cache}
rm -rf ~/.cache/matplotlib - if exists
```

Configuration

```
# ${MPLCONFIGDIR}/matplotlibrc
font.family: STSong
axes.unicode_minus: False
```

### jupyter {-}

Commands:

1. `juypter notebook --generate-config`: generate config file in directory `${JUPYTER_CONFIG_DIR}`
2. `jupyter notebook --no-browser --port=5000 --ip=0.0.0.0`: default port 8888
3. `jupyter notebook password`:  set jupyter login password, or use the following method
4. `python -c "from notebook.auth import passwd; print(passwd('jupyter'))"`

Configuration

```python
# ${JUPYTER_CONFIG_DIR}/jupyter_notebook_config.py
import os                                                                                 
import sys                                                                                
                                                                                          
#  for Mac, set browser value definitely; otherwise jupyter will not open browser automatically                                 
if sys.platform == "darwin":                                                              
    c.NotebookApp.browser = "Safari"
                                                    
c.NotebookApp.notebook_dir = "/server/proj/py-lrn"
#c.NotebookApp.password = "sha1:9e799b2236aa:01085662782c7813128637089192f836901b196d"
```

ipython-notebook.service

```ini
# /usr/lib/systemd/system/ipython-notebook.service
[Unit]
Description=Jupyter Notebook Server

[Service]
Type=simple
Environment="LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib"
Environment="PATH=/usr/local/bin:/usr/bin:$PATH"
ExecStart=/server/proj/.env/py-lrn/bin/jupyter notebook --ip=0.0.0.0 --no-browser                                                   
User=yang
Group=yang

[Install]
WantedBy=multi-user.target
```

## Database {#py-db}

1. MySQL
    * `mysqlclient`
2. PostgreSQL
    * `psycopg2-binary`
3. SQLServer
    * `pymssql`
    * dependencies
        * `freetds-devel`: for `sqlfont.h`
        * `unixODBC-devel`: for sql.h
        * `C_INCLUDE_PATH`: search `sql.h` and other header files

        ```sh
        yum install freetds-devel unixODBC-devel
        export C_INCLUDE_PATH=/usr/include
        ```
4. Hive
    * `pyhive[hive]`
    * dependencies
        * `thrift-sasl`: when meets "TSaslClientTransport' object has no attribute 'readAll'"

        ```sh
        yum install cyrus-sasl-devel cyrus-sasl-plain thrift-sasl
        ```
5. SQLAlchemy
    * connection string: `prefix://username:password@host:port/databasename?parameter=value`
    * prefix:
        * PostgreSQL: `postgresql+psycopg2`
        * MySQL: `mysql`
        * SQLServer: `mssql+pymssql`
    * parameter:
        * MySQL: `charset=utf8`

## Airlfow

### Installation {-}

1. `SLUGIFY_USES_TEXT_UNIDECODE=yes pip install apache-airflow`: since airflow >= 1.10
2. `pip install Fernet cryptography`
3. `pip install "apache-airflow[password]"`: for authentication
4. `pip install "apache-airflow[hive]"`: for hive
5. `pip install psycopg2-binary`: for postgresql
6. `pip install "apache-airflow[mssql]"`: for mssql
7. `pip install "apache-airflow[celery]"`: `4.4.7` for `1.9`
8. `pip install "apache-airflow[redis]"`: when use redis for CeleryExecutor
9. `pip install "apache-airflow[rabbitmq]"`: when use rabbitmq for CeleryExecutor

Installing `apache-airflow[rabbitmq]` may encounter a source error, then:

1. modify `except Exception, e` to `except Exception as e`
2. `python setup.py build`
3. `python setup.py install`

### Setup {-}

1. set `AIRFLOW_HOME` path

    ```sh
    export AIRFLOW_HOME= /YOUR/AIRFLOW/PROJECT/HOME/
    ```

2. initialization: 
 
    ```sh
    airflow initdb # < 2.0
    airflow db init # >= 2.0
    ```
    
    in generated file `airflow.cfg`, update following values according to demands:

    1. sql_alchemy_conn
        1. postgresql:  `sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/proj`
        2. mysql: `sql_alchemy_conn = mysql://airflow:airflow@localhost/airflow?charset=utf8`

    2. executor
        1. `executor = CeleryExecutor`
            1. use redis:
                1. without password: `broker_url = redis://127.0.0.1:6379/15`
                2. with password: `broker_url = redis://:password@127.0.0.1:6379/15`
            2. use rabbigmq: `broker_url = amqp://myairflow:airflowpass@localhost:5672/myvhost`

                ```sh
                # set rabbimq airflow user when using rabbitmq
                sudo rabbitmqctl add_user myairflow airflowpass
                sudo rabbitmqctl add_vhost myvhost
                sudo rabbitmqctl set_user_tags myairflow airflow
                sudo rabbitmqctl set_permissions -p myvhost myairflow ".*" ".*" ".*"
                ```

    3. result_backend (celery_result_backend < 1.10)
        1. postgresql: `result_backend = db+postgresql://airflow:airflow@localhost:5432/proj`
        2. mysql: `result_backend = db+mysql://airflow:airflow@localhost/airflow?charset=utf8`
        3. redis: `result_backend = redis://127.0.0.1:6379/15`

    4. authenticate under `[webserver]` part, not `[api]` part, otherwise it may be report all kinds of error, e.g., not found `client_auth`
        1. `<2.0`
            1. `authenticate = True`
            2. `auth_backend = airflow.contrib.auth.backends.password_auth`
        2. `>= 2.0`
            1. `auth_backend = airflow.api.auth.backend.basic_auth`
        
    5. timezone
        1. `default_timezone = Asia/Shanghai` (1.10)
        2. `default_ui_timezone = Asia/Shanghai` (>=2.0)

    6. `load_examples = False`: hide example dags, config before first start webserver, otherwise:
        1. manually delete example dag records in `dag` table
        2. `airflow resetdb`: rebuild database, but will lose other useful settings, avoid in production environment

    7. email under `[email]` part
        1. use `tls`

            ```ini
            smtp_starttls = True
            smtp_ssl = False
            smtp_port = 587
            ```
        2. use `ssl`

            ```ini
            smtp_starttls = False
            smtp_ssl = True
            smtp_port = 465
            ```
        
        3. `smtp_host`
            1. `smtp.exmail.qq.com`: for qq
        
        4. other settings

            ```ini
            smtp_user = hello@world.com
            smtp_password = password
            smtp_mail_from = hello@world.com
            ```

3. create airflow user account

    ```python
    # create_user.py
    import airflow
    from airflow import models, settings
    from airflow.contrib.auth.backends.password_auth import PasswordUser
    user = PasswordUser(models.User())
    user.username = 'airflow'
    user.email = 'new@example.com'
    # user.password = 'airflow'
    user._set_password = 'airflow'
    session = settings.Session()
    session.add(user)
    session.commit()
    session.close()
    ```

    ```sh
    python create_user.py
    ```
    or directly create user

    ```sh
    airflow users create \
        --username admin \
        --firstname Peter \
        --lastname Parker \
        --role Admin \
        --email spiderman@superhero.org
    ```

4. firewall
    1. `sudo firewall-cmd --permanent --add-port=8000/tcp`: airflow webserver, 8080 default 
    2. `sudo firewall-cmd --permanent --add-port=5555/tcp`: airflow flower, 5555 default
    3. `sudo systemctl reload firewalld`

### Deployment {-}

#### Start services directly {-}

1. start webserver with port `8080` default: `airflow webserver --port 8080`
2. start worker: `airflow worker`
3. start scheduler: `airflow scheduler`
4. start flower, with port `5555` default: `airflow flower`
   
```sh
nohup airflow webserver >> /data/jst/airflow/server_logs/webserver.log 2>&1 &
nohup airflow worker >> /data/jst/airflow/server_logs/worker.log 2>&1 &
nohup airflow scheduler >> /data/jst/airflow/server_logs/scheduler.log 2>&1 &
nohup airflow flower >> /data/jst/airflow/server_logs/flower.log 2>&1 &
```

#### Systemd {-}

Using systemd to deploy airflow server, and copy the following service file to `/usr/lib/systemd/system/`:

```sh
cp airflow-*er.service /usr/lib/systemd/system/
```

1. `airflow.conf` file

    ```
    D /run/airflow 0755 airflow airflow
    ```

    ```sh
    cp airflow.conf /etc/tmpfiles.d/
    mkdir /run/airflow
    chown airflow: /run/airflow
    ```

2. `airflow` file

    ```sh
    AIRFLOW_HOME=/server/proj/airflow/test
    SCHEDULER_RUNS=10
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib
    PYTHONPATH=${AIRFLOW_HOME}:${AIRFLOW_HOME}/plugins:${PYTHONPATH}
    PATH=/server/proj/.env/airflow/bin:/usr/bin:/user/local/bin:/usr/local/sbin:/usr/sbin:${PATH}
    ```

    ```sh
    cp airflow /etc/sysconfig/
    ```

3. `airflow-webserver.service` file
   
    ```ini
    [Unit]
    Description=Airflow webserver daemon 
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow webserver -p 8000 --pid /run/airflow/webserver.pid
    Restart=on-failure
    RestartSec=5s
    PrivateTmp=true

    [Install]
    WantedBy=multi-user.target
    ```

4. `airflow-worker.service` file

    ```ini
    [Unit]
    Description=Airflow celery worker daemon
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow worker
    Restart=on-failure
    RestartSec=10s

    [Install]
    WantedBy=multi-user.target
    ```

5. `airflow-scheduler.service` file

    ```ini
    [Unit]
    Description=Airflow scheduler daemon
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow scheduler
    Restart=always
    RestartSec=5s

    [Install]
    WantedBy=multi-user.target
    ```

6. `airflow-flower.service` file 

    ```ini
    [Unit]
    Description=Airflow celery flower
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow flower
    Restart=on-failure
    RestartSec=5s

    [Install]
    WantedBy=multi-user.target
    ```

### Concepts {-}

#### DAG {-}

In airflow, a `DAG` (Directed Acyclic Graph) is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.

DAGs are defined in standard python file placed in Airflow's `DAG_FOLDER`. Airflow will execute the code in each file to dynamically build the `DAG` objects.

If a dictionary of `default_args` is passed to a DAG, it will apply them to any of its operators. This makes it easy to apply a common parameter to many operators without having to type it many times.

1. `"owner" = "airflow"`: whom the tasks belong to, everything you can use
2. `"start_date": datetime(2020, 3, 15)`: the **execution date** start date
3. `"end_date": datetime(2020, 3, 20)`: the **execution date** end date
4. `"depends_on_past" = False"`: whether individual task instances will depend on the success of the preceding task instance, except for the `start_date` specified itself, for which this dependency is disregarded
5. `"email": ["airflow@example.com"]`: to who the mail sends to
6. `"email_on_failure": True`: send the mail when the task fails
7. `"email_on_retry: True`: whether retries when the task fails
8. `"retries": 5`: how many times to retry
9. `"retry_delay": timedelta(minutes=5)`: when to retry when the task fails
10. `"queue": "bash_queue"`: tasks belong to which queue
11. `"pool": "backfill"`: tasks belong to which pool
12. `"priority_weight": 10`

`schedule_interval` defines when to run the dag.

1. `* * * * *`: similar to cron format

```python
# tutorial.py
from airflow import DAG
from datetime import datetime, timedelta

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datatime(2020, 3, 15),
    "email": ["airflow@example.com],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG("tutorial", default_args = default_args, schedule_interval = "10 * * * *")
```

Since `1.8`, DAGs can be used as context managers to automatically assign new operators to that DAG.

```python
with DAG("tutorial", default_args=default_args) as dag:
    op = DummyOperator("op")

op.dag is dag # True
```

Dag operations:

1. `python tutorial.py`: test whether the dag pipeline parses
2. `airflow list_dags`: print the list of **active** dags
3. `airflow list_tasks tutorial`: print the list of tasks in the dag_id tutorial
4. `airflow list_tasks tutorial --tree`: print the hierarchy of tasks in the dag_id tutorial

#### Task {-}

Tasks are generated when instantiating operator objects. A task must include or inherit the
arguments `task_id` and `owner`, otherwise Airflow will raise an exception.

Arguments common for every tasks:

1. `task_id="task_name"`: the first argument for every opeartor
2. `owner="airflow"`: which owner the task belongs to, can be inherited from DAG's `default_args`
3. `dag=dag`: which dag the task belongs to

The dependencies between tasks can use:

1. `t2.set_upstream(t1)` <=> `dag.set_dependency("task_id_t1", "task_id_t2")` <=> `t2 << t1 (since 1.8)`
2. `t1.set_downstream(t2)` <=> `t1 >> t2 (since 1.8)`

Task test

`airflow test` command runs task instances locally, outputs their log to stdout (on screen), doesn’t bother with dependencies, and doesn’t communicate state (running, success, failed, …) to the database. It simply allows testing a single task instance.

The actual task instances run on a specific date. The date specified in this context is an `execution_date`, which simulates the scheduler running your task or dag at a specific date + time:

1. `airflow test tutorial print_date 2015-06-01`: test `print_date` on execution_date `2015-06-01`

Task run

Similar to task test, but runs as usual task

1. `airflow run tutorial print_date 2015-01-01`

Task backfill

The date range in this context is a start_date and optionally an end_date, which are used to populate the run schedule with task instances from this dag.

1. `airflow backfill tutorial -s 2015-06-01 -e 2015-06-07`

#### Operators {-}

Operators determines what actually gets done. It describes a single task in a workflow. Operators are only loaded by Airflow if they are assigned to a DAG.

Airflow provides operators for many common tasks, including:

1. `DummyOperator`:
   
   ```python
   from airflow.operators.dummy_operator import DummyOperator
   ```

2. `BashOperator`: execute a bash command
    1. `bash_command`: can be shell statements or a `.sh` file, or Jinja template. The file location is relative to the directory containing the pipeline file (`tutorial.py`)
    2. `params={"my_param": "parameter I passed in"}`: allow to pass a dictionary of parameters or objects to templates

    ```python
    from airflow.operators.bash_operator import BashOperator

    t1 = BashOperator(
        task_id="print_hello",
        bash_command="date",
        dag=dag
        )

    templated_command = """
        {% for i in range(5) %}
            echo "{{ ds }}"
            echo "{{ macros.ds_add(ds, 7) }}"
            echo "{{ params.my_param }}"
        {% endfor %}
        """

    t2 = BashOperator(
        task_id='templated',
        bash_command=templated_command,
        params={'my_param': 'Parameter I passed in'},
        dag=dag)
    
    t2.set_upstream(t1)
    ```

3. `PythonOperator`: calls an arbitary python function

    ```python
    from airflow.operators.python_operator import PythonOperator
    ```

4. `EmailOperator`: send an email

    ```python
    from airflow.operators.email_operator import EmailOperator
    ```
    
5. `HTTPOperator`: send an HTTP request
6. `SubDagOperator`

    ```python
    from airflow.operators.subdag_operator import SubDagOperator
    ```

7. `SSHOperator`

    ```python
    from airflow.contrib.operators.ssh_operator import SSHOperator
    ```
   
8. SQL Operators:
    1. `SqliteOperator`
    2. `PostgresOperator`
    3. `MySqlOperator`
    4. `MsSqlOperator`
    5. `OracleOperator`
    6. `JdbcOperator`
    7. `HiveOperator`
9.  `Sensor`: wait for a certain time, file, database row, etc...
   

#### Default Variables {-}

The Airflow engine passes a few variables by default that are accessible in all templates:

1. `{{ ds }}`: the execution date as `YYYY-MM-DD`
2. `{{ ds_nodash }}`: the execution date as `YYYYMMDD`
3. `{{ execution_date }}`: the execution_date, (datetime.datetime)
4. `{{ dag}}`: the DAG object
5. `{{ task }}`: the Task object, `{{ task.owner}}`, `{{ task.task_id}}`, ...
6. `{{ task_instance }}, {{ ti }}`: the task_instance object, `{{ ti.hostname}}`
7. `{{ task_instance_key_str }}`: a unique, human-readable key to task instance formatted `{dag_id}_{task_id}_{ds}`
8. `{{ params }}`: a reference to the user_defined params dictionary
9. `{{ conf }}`: the full configuration object located at `airflow.configuration.conf` which represents the content of your `airflow.cfg`
10. `{{ var.value.my_var }}`: global defined variables represented as a dictionary
11. `{{ var.json.my_var.path }}`: global defined variables represented as a dictionary with deserialized JSON object, append the path to the key within the JSON object
12. `{{ run_id }}`: the `run_id` of the current DAG run
13. `{{ dag_run }}`: a reference to the DagRun object
14. `{{ test_mode }}`: whether the task instance was called using the CLI’s `test` subcommand

#### Macros {-}

1. `macros.datetime`: The standard lib’s `datetime.datetime`
1. `macros.timedelta`: The standard lib’s `datetime.timedelta`
1. `macros.dateutil`: A reference to the `dateutil` package
1. `macros.time`: The standard lib’s `time`
1. `macros.uuid`: The standard lib’s `uuid`
1. `macros.random`: The standard lib’s `random`

### Operations {-}

1. `airflow backfill dag_name -s 2021-01-01 -e 2021-01-02`
2. `airflow run dag_name task_id 2021-01-01`

## Base {#py-base}

Functions

1. `id()`
2.  `sys.getsizeof() - return the size of objects in bytes`
3. `type()`
4. `repr()`
5. `str()`
6. `dir()`
7. `issubclass()`
8. `isinstance()`: `isinstance(True, int) => True`
9. `getattr()`
10. `hasattr()`
11. `class.__mro__`
12. `class.mro()`
13. `class.__bases__`
14. `os.path.abspath(__file__)`
15. `os.path.dirname()`
16. `os.path.join()`
17. `".".join()`

False value

1. `0; 0.0; 0j`
2. `""`
3. `False`
4. `[]`
5. `()`
6. `{}`
7. `None`

Packages

1. `http.server`: `python -m http.server 8888`
