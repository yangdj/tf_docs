# Python

## Environment Management

### venv {-}

`venv` module can be used to create virtual environment.

1. create virtual enviromeng

    ```sh
    python3 -m venv virtual-name
    ```

2. activate virtual environment

    ```sh
    source virtual-name/bin/activate; .virtual-name\scripts\activate
    ```

3. quit the environment

    ```sh
    deactivate
    ```

### pip {-}

`pip` is used to manage packages.

Configuration:

* `~/.pip/pip.conf`: for Linux and OSX
* `~\pip\pip.ini`: for Windows

```ini
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host = https://pypi.tuna.tsinghua.edu.cn
```

Commands:

1. `pip install package`
2. `pip uninstall package`
3. `pip install --upgrade package`
4. `pip list package`
5. `pip show package`
6. `pip search package`
7. `pip freeze > requirements.txt`
8. `pip install -r requirements.txt`

### Conda {-}

Virtual environment:

1. `conda --version`
2. `conda update conda`
3. `conda create --name vtl-name python=3.5`
4. `conda activate vtl-name`
5. `conda activate, conda deactivate`: switch back to default environment (base)
6. `conda info --envs`: list all environments
7. `conda remove --name vtl-name --all`: remove environment
8. `conda config --append channels conda-forge` - adds new channel when "packages not available ..."

Package management:

1. `conda install pkg(=1.1.1)`
2. `conda remove pkg`
3. `conda update pkg`
4. `conda list pkg`
5. `conda search pkg`

## Jupyter Deploy

### Package Dependent {-}

1. ipython
2. line_profiler
3. numpy
4. matplotlib
5. pandas
6. jupyter
7. scipy
8. scikit-learn
9. seaborn
10. tensorflow
11. keras

### Environment Variable {-}

```sh
export IPYTHONDIR=~/.ipython - default value
export MPLCONFIGDIR=~/.config/matplotlib - default value
export JUPYTER_CONFIG_DIR=~/.jupyter - default value
```

### ipython {-}

Commands:

1. `ipython profile create <name>`: create configuration file under `$IPYTHONDIR `
2. `ipython profile locate`
3. `ipython profile list`

Configuration: 

```python
# ${IPYTHONDIR}/profile_default/ipython_config.py
c.InteractiveShellApp.extensions = ["line_profiler"]
c.InteractiveShellApp.exec_lines = [
import numpy as np
import pandas as pd
]
```

### matplotlib {-}

Font install

```sh
mkdir -p /usr/share/fonts/STSong 
mv STSONG.ttf /usr/share/fonts/STSong
mkfontscale 
mkfontdir 
fc-cache -fv

cp STSONG.ttf site-packages/matplotlib/mpl-data/fonts/ttf
rm -rf ${MPLCONFIGDIR}/{fontList.json,tex.cache}
rm -rf ~/.cache/matplotlib - if exists
```

Configuration

```
# ${MPLCONFIGDIR}/matplotlibrc
font.family: STSong
axes.unicode_minus: False
```

### jupyter {-}

Commands:

1. `juypter notebook --generate-config`: generate config file in directory `${JUPYTER_CONFIG_DIR}`
2. `jupyter notebook --no-browser --port=5000 --ip=0.0.0.0`: default port 8888
3. `jupyter notebook password`:  set jupyter login password, or use the following method
4. `python -c "from notebook.auth import passwd; print(passwd('jupyter'))"`

Configuration

```python
# ${JUPYTER_CONFIG_DIR}/jupyter_notebook_config.py
import os                                                                                 
import sys                                                                                
                                                                                          
#  for Mac, set browser value definitely; otherwise jupyter will not open browser automatically                                 
if sys.platform == "darwin":                                                              
    c.NotebookApp.browser = "Safari"
                                                    
c.NotebookApp.notebook_dir = "/server/proj/py-lrn"
#c.NotebookApp.password = "sha1:9e799b2236aa:01085662782c7813128637089192f836901b196d"
```

ipython-notebook.service

```ini
# /usr/lib/systemd/system/ipython-notebook.service
[Unit]
Description=Jupyter Notebook Server

[Service]
Type=simple
Environment="LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib"
Environment="PATH=/usr/local/bin:/usr/bin:$PATH"
ExecStart=/server/proj/.env/py-lrn/bin/jupyter notebook --ip=0.0.0.0 --no-browser                                                   
User=yang
Group=yang

[Install]
WantedBy=multi-user.target
```

## Database {#py-db}

1. MySQL
    * `mysqlclient`
2. PostgreSQL
    * `psycopg2-binary`
3. SQLServer
    * `pymssql`
    * dependencies
        * `freetds-devel`: for `sqlfont.h`
        * `unixODBC-devel`: for sql.h
        * `C_INCLUDE_PATH`: search `sql.h` and other header files

        ```sh
        yum install freetds-devel unixODBC-devel
        export C_INCLUDE_PATH=/usr/include
        ```
4. Hive
    * `pyhive[hive]`
    * dependencies
        * `thrift-sasl`: when meets "TSaslClientTransport' object has no attribute 'readAll'"

        ```sh
        yum install cyrus-sasl-devel cyrus-sasl-plain thrift-sasl
        ```
5. SQLAlchemy
    * connection string: `prefix://username:password@host:port/databasename?parameter=value`
    * prefix:
        * PostgreSQL: `postgresql+psycopg2`
        * MySQL: `mysql`
        * SQLServer: `mssql+pymssql`
    * parameter:
        * MySQL: `charset=utf8`

## Airlfow

### Installation {-}

1. `SLUGIFY_USES_TEXT_UNIDECODE=yes pip install apache-airflow`: since airflow >= 1.10
2. `pip install Fernet cryptography`
3. `pip install "apache-airflow[password]"`: for authentication
4. `pip install "apache-airflow[hive]"`: for hive
5. `pip install psycopg2-binary`: for postgresql
6. `pip install "apache-airflow[mssql]"`: for mssql
7. `pip install "apache-airflow[celery]"`
8. `pip install "apache-airflow[redis]"`: when use redis for CeleryExecutor
9. `pip install "apache-airflow[rabbitmq]"`: when use rabbitmq for CeleryExecutor

Installing `apache-airflow[rabbitmq]` may encounter a source error, then:

1. modify `except Exception, e` to `except Exception as e`
2. `python setup.py build`
3. `python setup.py install`

### Setup {-}

1. set `AIRFLOW_HOME` path

    ```sh
    export AIRFLOW_HOME= /YOUR/AIRFLOW/PROJECT/HOME/
    ```

2. initialization: 
 
    ```sh
    airflow initdb # < 2.0
    airflow db init # >= 2.0
    ```
    
    in generated file `airflow.cfg`, update following values according to demands:

    1. sql_alchemy_conn
        1. postgresql:  `sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/proj`
        2. mysql: `sql_alchemy_conn = mysql://airflow:airflow@localhost/airflow?charset=utf8`

    2. executor
        1. `executor = CeleryExecutor`
            1. use redis: `broker_url = redis://127.0.0.1:6379/15`
            2. use rabbigmq: `broker_url = amqp://myairflow:airflowpass@localhost:5672/myvhost`

                ```sh
                # set rabbimq airflow user when using rabbitmq
                sudo rabbitmqctl add_user myairflow airflowpass
                sudo rabbitmqctl add_vhost myvhost
                sudo rabbitmqctl set_user_tags myairflow airflow
                sudo rabbitmqctl set_permissions -p myvhost myairflow ".*" ".*" ".*"
                ```

    3. result_backend (celery_result_backend < 1.10)
        1. postgresql: `result_backend = db+postgresql://airflow:airflow@localhost:5432/proj`
        2. mysql: `result_backend = db+mysql://airflow:airflow@localhost/airflow?charset=utf8`
        3. redis: `result_backend = redis://127.0.0.1:6379/15`

    4. authenticate under `[webserver]` part, otherwise it may be report all kinds of error, e.g., not found `client_auth`
        1. `<2.0`
            1. `authenticate = True`
            2. `auth_backend = airflow.contrib.auth.backends.password_auth`
        2. `>= 2.0`
            1. `auth_backend = airflow.api.auth.backend.basic_auth`
        
    5. timezone
        1. `default_timezone = Asia/Shanghai` (<2.0)
        2. `default_ui_timezone = Asia/Shanghai` (>=2.0)

3. create airflow user account

    ```python
    # create_user.py
    import airflow
    from airflow import models, settings
    from airflow.contrib.auth.backends.password_auth import PasswordUser
    user = PasswordUser(models.User())
    user.username = 'airflow'
    user.email = 'new@example.com'
    user.password = 'airflow'
    session = settings.Session()
    session.add(user)
    session.commit()
    session.close()
    ```

    ```sh
    python create_user.py
    ```
    or directly create user

    ```sh
    airflow users create \
        --username admin \
        --firstname Peter \
        --lastname Parker \
        --role Admin \
        --email spiderman@superhero.org
    ```

4. firewall
    1. `sudo firewall-cmd --permanent --add-port=8000/tcp`: airflow webserver, 8080 default 
    2. `sudo firewall-cmd --permanent --add-port=5555/tcp`: airflow flower, 5555 default
    3. `sudo systemctl reload firewalld`

### Deployment {-}

Start services directly:

1. start webserver: `airflow webserver --port 8080`
2. start scheduler: `airflow scheduler`

Using systemd to deploy airflow server, and copy the following service file to `/usr/lib/systemd/system/`:

```sh
cp airflow-*er.service /usr/lib/systemd/system/
```

1. `airflow.conf` file

    ```
    D /run/airflow 0755 airflow airflow
    ```

    ```sh
    cp airflow.conf /etc/tmpfiles.d/
    mkdir /run/airflow
    chown airflow: /run/airflow
    ```

2. `airflow` file

    ```sh
    AIRFLOW_HOME=/server/proj/airflow/test
    SCHEDULER_RUNS=10
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib
    PYTHONPATH=${AIRFLOW_HOME}:${AIRFLOW_HOME}/plugins:${PYTHONPATH}
    PATH=/server/proj/.env/airflow/bin:/usr/bin:/user/local/bin:/usr/local/sbin:/usr/sbin:${PATH}
    ```

    ```sh
    cp airflow /etc/sysconfig/
    ```

3. `airflow-webserver.service` file
   
    ```ini
    [Unit]
    Description=Airflow webserver daemon 
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow webserver -p 8000 --pid /run/airflow/webserver.pid
    Restart=on-failure
    RestartSec=5s
    PrivateTmp=true

    [Install]
    WantedBy=multi-user.target
    ```

4. `airflow-worker.service` file

    ```ini
    [Unit]
    Description=Airflow celery worker daemon
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow worker
    Restart=on-failure
    RestartSec=10s

    [Install]
    WantedBy=multi-user.target
    ```

5. `airflow-scheduler.service` file

    ```ini
    [Unit]
    Description=Airflow scheduler daemon
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow scheduler
    Restart=always
    RestartSec=5s

    [Install]
    WantedBy=multi-user.target
    ```

6. `airflow-flower.service` file 

    ```ini
    [Unit]
    Description=Airflow celery flower
    After=network.target postgresql.service redis.service
    Wants=postgresql.service redis.service

    [Service]
    EnvironmentFile=/etc/sysconfig/airflow
    User=airflow
    Group=airflow
    Type=simple
    ExecStart=/server/proj/.env/airflow/bin/airflow flower
    Restart=on-failure
    RestartSec=5s

    [Install]
    WantedBy=multi-user.target
    ```

## Base {#py-base}

Functions

1. `id()`
2.  `sys.getsizeof() - return the size of objects in bytes`
3. `type()`
4. `repr()`
5. `str()`
6. `dir()`
7. `issubclass()`
8. `isinstance()`: `isinstance(True, int) => True`
9. `getattr()`
10. `hasattr()`
11. `class.__mro__`
12. `class.mro()`
13. `class.__bases__`
14. `os.path.abspath(__file__)`
15. `os.path.dirname()`
16. `os.path.join()`
17. `".".join()`

False value

1. `0; 0.0; 0j`
2. `""`
3. `False`
4. `[]`
5. `()`
6. `{}`
7. `None`

Packages

1. `http.server`: `python -m http.server 8888`
